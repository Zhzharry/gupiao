"""
è‚¡ç¥¨ä»·æ ¼é¢„æµ‹æ¨¡å‹è°ƒä¼˜æ–¹æ¡ˆ
========================

é’ˆå¯¹50%å‡†ç¡®ç‡é—®é¢˜çš„å…¨é¢ä¼˜åŒ–æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ï¼š
1. æ•°æ®è´¨é‡æ”¹è¿›
2. ç‰¹å¾å·¥ç¨‹ä¼˜åŒ–
3. æ¨¡å‹æ¶æ„è°ƒæ•´
4. è®­ç»ƒç­–ç•¥æ”¹è¿›
5. è¯„ä¼°æŒ‡æ ‡ä¼˜åŒ–

ä¸»è¦æ”¹è¿›ç‚¹ï¼š
- æ›´å¥½çš„æ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹
- æ”¹è¿›çš„æŸå¤±å‡½æ•°ï¼ˆæ–¹å‘é¢„æµ‹+ä»·æ ¼é¢„æµ‹ï¼‰
- æ›´åˆç†çš„æ¨¡å‹æ¶æ„
- æ›´å¥½çš„è®­ç»ƒç­–ç•¥
- æ›´å…¨é¢çš„è¯„ä¼°æŒ‡æ ‡
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
try:
    import ta  # æŠ€æœ¯æŒ‡æ ‡åº“
except ImportError:
    print("è­¦å‘Š: taåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€å•æŠ€æœ¯æŒ‡æ ‡")
    ta = None
from datetime import datetime, timedelta
import warnings
import os
warnings.filterwarnings('ignore')

class ImprovedStockDataProcessor:
    """æ”¹è¿›çš„æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, seq_length=30, prediction_days=1):
        self.seq_length = seq_length
        self.prediction_days = prediction_days
        self.feature_scaler = RobustScaler()  # ä½¿ç”¨RobustScalerï¼Œå¯¹å¼‚å¸¸å€¼æ›´ç¨³å¥
        self.target_scaler = StandardScaler()
        self.feature_columns = []
        
    def add_advanced_features(self, df):
        """æ·»åŠ é«˜çº§ç‰¹å¾å·¥ç¨‹"""
        df = df.copy()
        
        # åŸºç¡€ä»·æ ¼ç‰¹å¾
        df['return_1d'] = df['close'].pct_change()
        df['return_5d'] = df['close'].pct_change(5)
        df['return_10d'] = df['close'].pct_change(10)
        
        # ä»·æ ¼ä½ç½®ç‰¹å¾
        df['price_position_5d'] = (df['close'] - df['close'].rolling(5).min()) / (df['close'].rolling(5).max() - df['close'].rolling(5).min())
        df['price_position_20d'] = (df['close'] - df['close'].rolling(20).min()) / (df['close'].rolling(20).max() - df['close'].rolling(20).min())
        
        # æ³¢åŠ¨ç‡ç‰¹å¾
        df['volatility_5d'] = df['return_1d'].rolling(5).std()
        df['volatility_20d'] = df['return_1d'].rolling(20).std()
        
        # æˆäº¤é‡ç‰¹å¾
        df['volume_ma_5'] = df['vol'].rolling(5).mean()
        df['volume_ma_20'] = df['vol'].rolling(20).mean()
        df['volume_ratio'] = df['vol'] / df['volume_ma_20']
        
        # ä»·æ ¼çªç ´ç‰¹å¾
        df['breakout_up'] = (df['close'] > df['close'].rolling(20).max().shift(1)).astype(int)
        df['breakout_down'] = (df['close'] < df['close'].rolling(20).min().shift(1)).astype(int)
        
        # æŠ€æœ¯æŒ‡æ ‡ - ä½¿ç”¨ç®€å•è®¡ç®—æ–¹æ³•
        # RSIè®¡ç®—
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df['rsi'] = 100 - (100 / (1 + rs))
        
        # MACDè®¡ç®—
        ema12 = df['close'].ewm(span=12).mean()
        ema26 = df['close'].ewm(span=26).mean()
        df['macd'] = ema12 - ema26
        df['macd_signal'] = df['macd'].ewm(span=9).mean()
        
        # å¸ƒæ—å¸¦è®¡ç®—
        sma20 = df['close'].rolling(window=20).mean()
        std20 = df['close'].rolling(window=20).std()
        df['bb_upper'] = sma20 + (std20 * 2)
        df['bb_lower'] = sma20 - (std20 * 2)
        df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['close']
        
        # ç§»åŠ¨å¹³å‡çº¿
        df['sma_5'] = df['close'].rolling(5).mean()
        df['sma_20'] = df['close'].rolling(20).mean()
        df['ema_12'] = df['close'].ewm(span=12).mean()
        df['ema_26'] = df['close'].ewm(span=26).mean()
        
        # è¶‹åŠ¿ç‰¹å¾
        df['trend_5d'] = (df['close'] > df['sma_5']).astype(int)
        df['trend_20d'] = (df['close'] > df['sma_20']).astype(int)
        
        # å¸‚åœºçŠ¶æ€ç‰¹å¾
        df['market_cap'] = df['close'] * df['vol']  # å‡è®¾å¸‚å€¼
        df['price_volume_trend'] = df['close'] * df['vol']
        
        return df
    
    def create_labels(self, df):
        """åˆ›å»ºå¤šä»»åŠ¡æ ‡ç­¾"""
        labels = {}
        
        # ä»·æ ¼é¢„æµ‹æ ‡ç­¾ï¼ˆå›å½’ï¼‰
        labels['price'] = df['close'].shift(-self.prediction_days)
        
        # æ–¹å‘é¢„æµ‹æ ‡ç­¾ï¼ˆåˆ†ç±»ï¼‰
        future_return = df['close'].shift(-self.prediction_days) / df['close'] - 1
        labels['direction'] = (future_return > 0).astype(int)
        
        # å¹…åº¦é¢„æµ‹æ ‡ç­¾ï¼ˆåˆ†ç±»ï¼‰
        labels['magnitude'] = pd.cut(future_return, 
                                   bins=[-np.inf, -0.02, 0.02, np.inf], 
                                   labels=[0, 1, 2])  # 0:ä¸‹è·Œ, 1:æ¨ªç›˜, 2:ä¸Šæ¶¨
        
        return labels
    
    def prepare_sequences(self, df, labels):
        """å‡†å¤‡åºåˆ—æ•°æ®"""
        # é€‰æ‹©ç‰¹å¾åˆ— - æ’é™¤æ—¥æœŸã€ä»£ç ç­‰éæ•°å€¼åˆ—
        exclude_cols = ['date', 'code', 'tradingday', 'secucode', 'ts_code']  # æ·»åŠ æ›´å¤šå¯èƒ½çš„éæ•°å€¼åˆ—å
        feature_cols = [col for col in df.columns if col not in exclude_cols]
        
        # è¿›ä¸€æ­¥è¿‡æ»¤ï¼šåªä¿ç•™æ•°å€¼ç±»å‹çš„åˆ—
        numeric_cols = []
        for col in feature_cols:
            if df[col].dtype in ['int64', 'float64', 'int32', 'float32']:
                numeric_cols.append(col)
            else:
                print(f"è·³è¿‡éæ•°å€¼åˆ—: {col} (ç±»å‹: {df[col].dtype})")
        
        self.feature_columns = numeric_cols
        print(f"ä½¿ç”¨çš„ç‰¹å¾åˆ—æ•°é‡: {len(numeric_cols)}")
        
        # åªé€‰æ‹©æ•°å€¼åˆ—
        df_numeric = df[numeric_cols].copy()
        
        # ç§»é™¤åŒ…å«NaNçš„è¡Œ
        df_clean = df_numeric.dropna()
        print(f"æ¸…ç†åçš„æ•°æ®å½¢çŠ¶: {df_clean.shape}")
        
        # å¯¹é½æ ‡ç­¾
        for key in labels:
            labels[key] = labels[key].loc[df_clean.index]
        
        # æ ‡å‡†åŒ–ç‰¹å¾ - ç°åœ¨åªå¤„ç†æ•°å€¼æ•°æ®
        features_scaled = self.feature_scaler.fit_transform(df_clean)
        
        # åˆ›å»ºåºåˆ—
        X, y_price, y_direction, y_magnitude = [], [], [], []
        
        for i in range(len(features_scaled) - self.seq_length - self.prediction_days + 1):
            if not pd.isna(labels['price'].iloc[i + self.seq_length - 1]):
                X.append(features_scaled[i:i + self.seq_length])
                y_price.append(labels['price'].iloc[i + self.seq_length - 1])
                y_direction.append(labels['direction'].iloc[i + self.seq_length - 1])
                y_magnitude.append(labels['magnitude'].iloc[i + self.seq_length - 1])
        
        print(f"æœ€ç»ˆåºåˆ—æ•°é‡: {len(X)}")
        return np.array(X), np.array(y_price), np.array(y_direction), np.array(y_magnitude)

class ImprovedStockTransformer(nn.Module):
    """æ”¹è¿›çš„Transformeræ¨¡å‹"""
    
    def __init__(self, input_dim, d_model=128, nhead=8, num_layers=3, seq_len=30, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.seq_len = seq_len
        
        # è¾“å…¥æŠ•å½±
        self.input_projection = nn.Linear(input_dim, d_model)
        
        # ä½ç½®ç¼–ç 
        self.pos_encoding = self.create_positional_encoding(seq_len, d_model)
        
        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,
            dropout=dropout,
            activation='gelu'
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # å¤šå¤´è¾“å‡º
        self.price_head = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, 1)
        )
        
        self.direction_head = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, 2)  # ä¸Šæ¶¨/ä¸‹è·Œ
        )
        
        self.magnitude_head = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, 3)  # ä¸‹è·Œ/æ¨ªç›˜/ä¸Šæ¶¨
        )
        
        # æ³¨æ„åŠ›æ± åŒ–
        self.attention_pool = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))
        
    def create_positional_encoding(self, seq_len, d_model):
        """åˆ›å»ºä½ç½®ç¼–ç """
        pe = torch.zeros(seq_len, d_model)
        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        return pe.unsqueeze(0)
    
    def forward(self, x):
        batch_size = x.size(0)
        
        # è¾“å…¥æŠ•å½±
        x = self.input_projection(x)  # [batch, seq, d_model]
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.pos_encoding[:, :x.size(1), :].to(x.device)
        
        # æ·»åŠ CLS token
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        x = torch.cat([cls_tokens, x], dim=1)
        
        # Transformerç¼–ç 
        x = x.transpose(0, 1)  # [seq+1, batch, d_model]
        x = self.transformer(x)
        
        # ä½¿ç”¨CLS tokençš„è¾“å‡º
        cls_output = x[0]  # [batch, d_model]
        
        # å¤šå¤´è¾“å‡º
        price_output = self.price_head(cls_output)
        direction_output = self.direction_head(cls_output)
        magnitude_output = self.magnitude_head(cls_output)
        
        return price_output, direction_output, magnitude_output

class MultiTaskLoss(nn.Module):
    """å¤šä»»åŠ¡æŸå¤±å‡½æ•°"""
    
    def __init__(self, alpha=1.0, beta=1.0, gamma=1.0):
        super().__init__()
        self.alpha = alpha  # ä»·æ ¼é¢„æµ‹æƒé‡
        self.beta = beta    # æ–¹å‘é¢„æµ‹æƒé‡
        self.gamma = gamma  # å¹…åº¦é¢„æµ‹æƒé‡
        
        self.mse_loss = nn.MSELoss()
        self.ce_loss = nn.CrossEntropyLoss()
        
    def forward(self, price_pred, direction_pred, magnitude_pred, 
                price_true, direction_true, magnitude_true):
        
        # ä»·æ ¼é¢„æµ‹æŸå¤±
        price_loss = self.mse_loss(price_pred.squeeze(), price_true)
        
        # æ–¹å‘é¢„æµ‹æŸå¤±
        direction_loss = self.ce_loss(direction_pred, direction_true.long())
        
        # å¹…åº¦é¢„æµ‹æŸå¤±ï¼ˆå¤„ç†å¯èƒ½çš„NaNï¼‰
        valid_mask = ~torch.isnan(magnitude_true)
        if valid_mask.sum() > 0:
            magnitude_loss = self.ce_loss(magnitude_pred[valid_mask], magnitude_true[valid_mask].long())
        else:
            magnitude_loss = torch.tensor(0.0, device=price_pred.device)
        
        # æ€»æŸå¤±
        total_loss = self.alpha * price_loss + self.beta * direction_loss + self.gamma * magnitude_loss
        
        return total_loss, price_loss, direction_loss, magnitude_loss

class ImprovedStockTrainer:
    """æ”¹è¿›çš„è‚¡ç¥¨è®­ç»ƒå™¨"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.processor = ImprovedStockDataProcessor(
            seq_length=config['seq_length'],
            prediction_days=config['prediction_days']
        )
        
    def train_model(self, train_data):
        """è®­ç»ƒæ¨¡å‹"""
        print("ğŸš€ å¼€å§‹æ”¹è¿›ç‰ˆæ¨¡å‹è®­ç»ƒ...")
        
        # æ•°æ®é¢„å¤„ç†
        print("ğŸ“Š æ•°æ®é¢„å¤„ç†...")
        train_data = self.processor.add_advanced_features(train_data)
        labels = self.processor.create_labels(train_data)
        X, y_price, y_direction, y_magnitude = self.processor.prepare_sequences(train_data, labels)
        
        print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: {X.shape[0]} ä¸ªæ ·æœ¬")
        print(f"   ç‰¹å¾ç»´åº¦: {X.shape[2]}")
        print(f"   åºåˆ—é•¿åº¦: {X.shape[1]}")
        
        # æ•°æ®åˆ’åˆ†
        split_idx = int(0.8 * len(X))
        X_train, X_val = X[:split_idx], X[split_idx:]
        y_price_train, y_price_val = y_price[:split_idx], y_price[split_idx:]
        y_direction_train, y_direction_val = y_direction[:split_idx], y_direction[split_idx:]
        y_magnitude_train, y_magnitude_val = y_magnitude[:split_idx], y_magnitude[split_idx:]
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataset = TensorDataset(
            torch.FloatTensor(X_train),
            torch.FloatTensor(y_price_train),
            torch.LongTensor(y_direction_train),
            torch.LongTensor(y_magnitude_train)
        )
        
        train_loader = DataLoader(
            train_dataset, 
            batch_size=self.config['batch_size'],
            shuffle=True,
            pin_memory=True
        )
        
        # åˆ›å»ºæ¨¡å‹
        model = ImprovedStockTransformer(
            input_dim=X.shape[2],
            d_model=self.config['d_model'],
            nhead=self.config['nhead'],
            num_layers=self.config['num_layers'],
            seq_len=self.config['seq_length'],
            dropout=self.config['dropout']
        ).to(self.device)
        
        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = MultiTaskLoss(alpha=1.0, beta=2.0, gamma=1.0)  # å¼ºè°ƒæ–¹å‘é¢„æµ‹
        optimizer = optim.AdamW(
            model.parameters(), 
            lr=self.config['learning_rate'],
            weight_decay=self.config['weight_decay']
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = optim.lr_scheduler.OneCycleLR(
            optimizer, 
            max_lr=self.config['learning_rate'],
            steps_per_epoch=len(train_loader),
            epochs=self.config['epochs'],
            pct_start=0.3
        )
        
        # è®­ç»ƒå¾ªç¯
        train_losses = []
        val_accuracies = []
        best_val_acc = 0
        
        for epoch in range(self.config['epochs']):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            epoch_losses = []
            
            for batch_X, batch_y_price, batch_y_direction, batch_y_magnitude in train_loader:
                batch_X = batch_X.to(self.device)
                batch_y_price = batch_y_price.to(self.device)
                batch_y_direction = batch_y_direction.to(self.device)
                batch_y_magnitude = batch_y_magnitude.to(self.device)
                
                optimizer.zero_grad()
                
                price_pred, direction_pred, magnitude_pred = model(batch_X)
                
                total_loss, price_loss, direction_loss, magnitude_loss = criterion(
                    price_pred, direction_pred, magnitude_pred,
                    batch_y_price, batch_y_direction, batch_y_magnitude
                )
                
                total_loss.backward()
                optimizer.step()
                scheduler.step()
                
                epoch_losses.append(total_loss.item())
            
            # éªŒè¯é˜¶æ®µ
            model.eval()
            val_predictions = []
            val_targets = []
            
            with torch.no_grad():
                val_X = torch.FloatTensor(X_val).to(self.device)
                val_y_direction = torch.LongTensor(y_direction_val).to(self.device)
                
                # æ‰¹é‡é¢„æµ‹ä»¥èŠ‚çœå†…å­˜
                batch_size = self.config['batch_size']
                for i in range(0, len(val_X), batch_size):
                    batch_val_X = val_X[i:i+batch_size]
                    _, direction_pred, _ = model(batch_val_X)
                    val_predictions.extend(direction_pred.argmax(dim=1).cpu().numpy())
                    val_targets.extend(val_y_direction[i:i+batch_size].cpu().numpy())
            
            # è®¡ç®—éªŒè¯å‡†ç¡®ç‡
            val_acc = accuracy_score(val_targets, val_predictions)
            val_accuracies.append(val_acc)
            
                # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                torch.save(model.state_dict(), 'best_improved_model.pth')
            
            # æ‰“å°è¿›åº¦
            avg_loss = np.mean(epoch_losses)
            train_losses.append(avg_loss)
            
            print(f'Epoch [{epoch+1}/{self.config["epochs"]}]')
            print(f'  Loss: {avg_loss:.4f}')
            print(f'  Val Accuracy: {val_acc:.4f}')
            print(f'  Best Val Accuracy: {best_val_acc:.4f}')
            print(f'  LR: {scheduler.get_last_lr()[0]:.6f}')
            print('-' * 50)
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        model.load_state_dict(torch.load('best_improved_model.pth'))
        
        # æœ€ç»ˆè¯„ä¼°
        self.evaluate_model(model, X_val, y_price_val, y_direction_val, y_magnitude_val)
        
        return model
    
    def evaluate_model(self, model, X_test, y_price_test, y_direction_test, y_magnitude_test):
        """è¯„ä¼°æ¨¡å‹"""
        model.eval()
        
        with torch.no_grad():
            X_test_tensor = torch.FloatTensor(X_test).to(self.device)
            
            # æ‰¹é‡é¢„æµ‹
            price_predictions = []
            direction_predictions = []
            magnitude_predictions = []
            
            batch_size = self.config['batch_size']
            for i in range(0, len(X_test_tensor), batch_size):
                batch_X = X_test_tensor[i:i+batch_size]
                price_pred, direction_pred, magnitude_pred = model(batch_X)
                
                price_predictions.extend(price_pred.cpu().numpy().flatten())
                direction_predictions.extend(direction_pred.argmax(dim=1).cpu().numpy())
                magnitude_predictions.extend(magnitude_pred.argmax(dim=1).cpu().numpy())
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        direction_acc = accuracy_score(y_direction_test, direction_predictions)
        direction_precision = precision_score(y_direction_test, direction_predictions, average='weighted')
        direction_recall = recall_score(y_direction_test, direction_predictions, average='weighted')
        direction_f1 = f1_score(y_direction_test, direction_predictions, average='weighted')
        
        # ä»·æ ¼é¢„æµ‹è¯„ä¼°
        price_mse = np.mean((y_price_test - price_predictions) ** 2)
        price_mae = np.mean(np.abs(y_price_test - price_predictions))
        
        print("\nğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ:")
        print("=" * 50)
        print(f"ğŸ¯ æ–¹å‘é¢„æµ‹å‡†ç¡®ç‡: {direction_acc:.4f}")
        print(f"ğŸ“ˆ æ–¹å‘é¢„æµ‹ç²¾ç¡®ç‡: {direction_precision:.4f}")
        print(f"ğŸ“Š æ–¹å‘é¢„æµ‹å¬å›ç‡: {direction_recall:.4f}")
        print(f"ğŸª æ–¹å‘é¢„æµ‹F1åˆ†æ•°: {direction_f1:.4f}")
        print(f"ğŸ’° ä»·æ ¼é¢„æµ‹MSE: {price_mse:.6f}")
        print(f"ğŸ’² ä»·æ ¼é¢„æµ‹MAE: {price_mae:.6f}")
        print("=" * 50)
        
        return {
            'direction_accuracy': direction_acc,
            'direction_precision': direction_precision,
            'direction_recall': direction_recall,
            'direction_f1': direction_f1,
            'price_mse': price_mse,
            'price_mae': price_mae
        }

# ä½¿ç”¨ç¤ºä¾‹å’Œé…ç½®
def main():
    """ä¸»å‡½æ•° - ä½¿ç”¨æ”¹è¿›çš„é…ç½®"""
    
    print("ğŸ¯ è‚¡ç¥¨ä»·æ ¼é¢„æµ‹æ¨¡å‹è®­ç»ƒç¨‹åº")
    print("=" * 60)
    
    # ä¼˜åŒ–åçš„é…ç½®
    config = {
    # è¾“å…¥åºåˆ—å‚æ•°
    'seq_length': 60,           # æ‰©å¤§å†å²çª—å£ï¼ˆæ•è·æ›´é•¿ä¾èµ–ï¼‰
    'prediction_days': 7,        # æ”¯æŒå¤šæ­¥é¢„æµ‹
    
    # æ¨¡å‹ç»“æ„å‚æ•°ï¼ˆæ ¸å¿ƒè°ƒæ•´ï¼‰
    'd_model': 256,              # åŸ128â†’256ï¼ˆæå‡ç‰¹å¾è¡¨è¾¾èƒ½åŠ›ï¼‰
    'nhead': 8,                  # ä¿æŒä¸d_modelå…¼å®¹ï¼ˆ256/8=32ï¼‰
    'num_layers': 4,             # åŸ3â†’4ï¼ˆåŠ æ·±ç½‘ç»œä½†é¿å…è¿‡æ·±ï¼‰
    'dim_feedforward': 1024,     # FFNå±‚ç»´åº¦ï¼ˆd_modelçš„4å€ï¼‰
    'dropout': 0.15,             # åŸ0.1â†’0.15ï¼ˆé€‚åº¦æ­£åˆ™åŒ–ï¼‰
    
    # è®­ç»ƒå‚æ•°ï¼ˆé€‚é…æ‰©å¤§åçš„æ¨¡å‹ï¼‰
    'batch_size': 64,            # åŸ32â†’64ï¼ˆæå‡å¹¶è¡Œæ•ˆç‡ï¼‰
    'learning_rate': 5e-4,       # åŸ1e-3â†’5e-4ï¼ˆå¹³è¡¡é€Ÿåº¦å’Œç¨³å®šæ€§ï¼‰
    'epochs': 150,               # å»¶é•¿è®­ç»ƒæ—¶é—´
    'weight_decay': 1e-5,        # æ›´å°çš„L2çº¦æŸï¼ˆé¿å…é™åˆ¶å¤§æ¨¡å‹ï¼‰
    'gradient_clip': 0.5,        # æ–°å¢æ¢¯åº¦è£å‰ªï¼ˆé˜²æ¢¯åº¦çˆ†ç‚¸ï¼‰
    
    # å¯é€‰æ‰©å±•
    'use_learning_rate_scheduler': True,  # å¯ç”¨å­¦ä¹ ç‡åŠ¨æ€è°ƒæ•´
    'patience': 10,              # æ—©åœè€å¿ƒå€¼
}
    
    print("ğŸ“‹ é…ç½®å‚æ•°:")
    for key, value in config.items():
        print(f"   {key}: {value}")
    print()
    
    # åˆ›å»ºè®­ç»ƒå™¨
    print("ğŸ—ï¸  æ­£åœ¨åˆ›å»ºè®­ç»ƒå™¨...")
    trainer = ImprovedStockTrainer(config)
    print("âœ… è®­ç»ƒå™¨åˆ›å»ºå®Œæˆ")
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    print("\nğŸ“Š æ­£åœ¨åŠ è½½è®­ç»ƒæ•°æ®...")
    train_data_dir = "../../data/learn_csv"
    
    if not os.path.exists(train_data_dir):
        print(f"âŒ è®­ç»ƒæ•°æ®ç›®å½•ä¸å­˜åœ¨: {train_data_dir}")
        print("è¯·ç¡®ä¿æ•°æ®å·²æ ‡å‡†åŒ–å¹¶æ”¾åœ¨æ­£ç¡®ä½ç½®")
        return
    
    # è·å–æ‰€æœ‰CSVæ–‡ä»¶
    csv_files = [f for f in os.listdir(train_data_dir) if f.endswith('.csv')]
    if not csv_files:
        print(f"âŒ åœ¨ {train_data_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°CSVæ–‡ä»¶")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
    
    # åŠ è½½ç¬¬ä¸€ä¸ªæ–‡ä»¶ä½œä¸ºç¤ºä¾‹ï¼ˆä½ å¯ä»¥ä¿®æ”¹ä¸ºåŠ è½½å¤šä¸ªæ–‡ä»¶ï¼‰
    sample_file = csv_files[0]
    sample_path = os.path.join(train_data_dir, sample_file)
    print(f"ğŸ“„ åŠ è½½ç¤ºä¾‹æ–‡ä»¶: {sample_file}")
    
    try:
        # åŠ è½½æ•°æ®
        train_data = pd.read_csv(sample_path)
        print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®ï¼Œå½¢çŠ¶: {train_data.shape}")
        print(f"ğŸ“Š æ•°æ®åˆ—: {list(train_data.columns)}")
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—
        required_cols = ['tradingday', 'secucode', 'close', 'open', 'high', 'low', 'vol', 'amount']
        missing_cols = [col for col in required_cols if col not in train_data.columns]
        if missing_cols:
            print(f"âš ï¸  ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_cols}")
            print("è¯·ç¡®ä¿æ•°æ®åŒ…å«å¿…è¦çš„ä»·æ ¼å’Œäº¤æ˜“ä¿¡æ¯")
            return
        
        # æ•°æ®é¢„å¤„ç†
        print("\nğŸ”§ æ•°æ®é¢„å¤„ç†...")
        # ç¡®ä¿æ—¥æœŸåˆ—æ ¼å¼æ­£ç¡®
        if 'tradingday' in train_data.columns:
            train_data['tradingday'] = pd.to_datetime(train_data['tradingday'], format='%Y%m%d')
            train_data = train_data.sort_values('tradingday')
        
        # ç§»é™¤é‡å¤æ•°æ®
        train_data = train_data.drop_duplicates()
        print(f"âœ… é¢„å¤„ç†å®Œæˆï¼Œæœ€ç»ˆæ•°æ®å½¢çŠ¶: {train_data.shape if train_data is not None else 'None'}")
        
        # å¼€å§‹è®­ç»ƒ
        print("\nğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
        model = trainer.train_model(train_data)
        
        print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print("=" * 60)
        print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: best_improved_model.pth")
        print("=" * 60)
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        print("è¯·æ£€æŸ¥æ•°æ®æ ¼å¼å’Œè·¯å¾„")
        return

if __name__ == "__main__":
    main()