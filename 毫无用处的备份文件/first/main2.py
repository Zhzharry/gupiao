"""
è‚¡ç¥¨ä»·æ ¼é¢„æµ‹æ¨¡å‹ - ä¸»è®­ç»ƒç¨‹åº (ä¿®å¤ç‰ˆ)
====================================

ä¿®å¤å†…å®¹ï¼š
1. ä¿®å¤ train_test_split è¿”å›å€¼è§£åŒ…é—®é¢˜
2. æ·»åŠ  scaler ä¿å­˜å’ŒåŠ è½½åŠŸèƒ½
3. æ”¹è¿›æ¨¡å‹ä¿å­˜æœºåˆ¶ï¼ŒåŒæ—¶ä¿å­˜ processor
4. ä¼˜åŒ–ä»£ç ç»“æ„å’Œé”™è¯¯å¤„ç†

ä½œè€…ï¼šAI Assistant
åˆ›å»ºæ—¶é—´ï¼š2024å¹´
"""

# å¯¼å…¥å¿…è¦çš„åº“
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import os
import pandas as pd
from datetime import datetime
import warnings
import pickle  # ç”¨äºä¿å­˜ processor
warnings.filterwarnings('ignore')
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from learn.models.transformer_model import StockTransformer, AdvancedStockTransformer, create_model
from data.data_processor import StockDataProcessor

class StockTrainer:
    """è‚¡ç¥¨è®­ç»ƒå™¨ç±»ï¼Œè´Ÿè´£æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°"""
    
    def __init__(self, config):
        """åˆå§‹åŒ–è®­ç»ƒå™¨"""
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        os.makedirs('../../models', exist_ok=True)
        os.makedirs('../../results', exist_ok=True)
        os.makedirs('../../logs', exist_ok=True)
        
    def load_data(self):
        """åŠ è½½å’Œå¤„ç†è®­ç»ƒæ•°æ®"""
        print("ğŸ“‚ æ­£åœ¨åŠ è½½è®­ç»ƒæ•°æ®...")
        
        # åˆ›å»ºæ•°æ®å¤„ç†å™¨å®ä¾‹
        print("ğŸ”§ æ­£åœ¨åˆ›å»ºæ•°æ®å¤„ç†å™¨...")
        processor = StockDataProcessor(seq_length=self.config['seq_length'])
        print(f"âœ… æ•°æ®å¤„ç†å™¨åˆ›å»ºå®Œæˆï¼Œåºåˆ—é•¿åº¦: {self.config['seq_length']}")
        
        # åŠ è½½è®­ç»ƒæ•°æ®
        train_data_dir = "../../data/learn_csv"
        if os.path.exists(train_data_dir):
            print(f"ğŸ“ æ‰¾åˆ°è®­ç»ƒæ•°æ®ç›®å½•: {train_data_dir}")
            train_stock_data = processor.load_multiple_stocks(
                train_data_dir, 
                stock_codes=self.config.get('stock_codes', None)
            )
            print(f"âœ… æˆåŠŸåŠ è½½äº† {len(train_stock_data)} åªè‚¡ç¥¨çš„è®­ç»ƒæ•°æ®")
        else:
            print(f"âŒ è®­ç»ƒæ•°æ®ç›®å½•ä¸å­˜åœ¨: {train_data_dir}")
            return None, None, None
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        print("ğŸ”„ æ­£åœ¨å‡†å¤‡è®­ç»ƒæ•°æ®...")
        X_train, y_train = processor.prepare_multi_stock_data(
            train_stock_data, 
            target_col=self.config['target_col']
        )
        
        if X_train is None:
            print("âŒ è®­ç»ƒæ•°æ®å‡†å¤‡å¤±è´¥")
            return None, None, None
        
        print(f"âœ… è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæˆ")
        if X_train is not None and y_train is not None:
            print(f"   ğŸ“Š ç‰¹å¾æ•°æ®å½¢çŠ¶: {X_train.shape if hasattr(X_train, 'shape') else 'unknown'}")
            print(f"   ğŸ“Š æ ‡ç­¾æ•°æ®å½¢çŠ¶: {y_train.shape if hasattr(y_train, 'shape') else 'unknown'}")
        else:
            print("   ğŸ“Š æ•°æ®å½¢çŠ¶: æ•°æ®ä¸ºNone")
            
        return X_train, y_train, processor
    
    def create_model(self):
        """åˆ›å»ºTransformeræ¨¡å‹"""
        model_type = self.config.get('model_type', 'basic')
        
        if model_type == 'basic':
            model = StockTransformer(
                input_dim=self.config['input_dim'],
                d_model=self.config['d_model'],
                nhead=self.config['nhead'],
                num_layers=self.config['num_layers'],
                seq_len=self.config['seq_length'],
                output_dim=1,
                dropout=self.config['dropout']
            )
        elif model_type == 'advanced':
            model = AdvancedStockTransformer(
                input_dim=self.config['input_dim'],
                d_model=self.config['d_model'],
                nhead=self.config['nhead'],
                num_layers=self.config['num_layers'],
                seq_len=self.config['seq_length'],
                output_dim=1,
                dropout=self.config['dropout']
            )
        else:
            model = create_model(model_type, **self.config)
        
        return model.to(self.device)
    
    def save_model_and_processor(self, model, processor, model_path="../../models/best_model.pth"):
        """ä¿å­˜æ¨¡å‹å’Œæ•°æ®å¤„ç†å™¨"""
        try:
            # ä¿å­˜æ¨¡å‹çŠ¶æ€å­—å…¸
            torch.save(model.state_dict(), model_path)
            print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
            
            # ä¿å­˜æ•°æ®å¤„ç†å™¨
            processor_path = model_path.replace('.pth', '_processor.pkl')
            with open(processor_path, 'wb') as f:
                pickle.dump(processor, f)
            print(f"âœ… æ•°æ®å¤„ç†å™¨å·²ä¿å­˜åˆ°: {processor_path}")
            
            # ä¿å­˜é…ç½®æ–‡ä»¶
            config_path = model_path.replace('.pth', '_config.pkl')
            with open(config_path, 'wb') as f:
                pickle.dump(self.config, f)
            print(f"âœ… é…ç½®æ–‡ä»¶å·²ä¿å­˜åˆ°: {config_path}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ¨¡å‹æ—¶å‡ºé”™: {e}")
    
    def train(self, X_train, y_train, processor):
        """è®­ç»ƒæ¨¡å‹"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        print("=" * 60)
        
        # å†…å­˜ä¼˜åŒ–
        torch.cuda.empty_cache()
        torch.backends.cudnn.benchmark = True
        
        # ä¿®å¤ï¼šæ­£ç¡®æ¥æ”¶ train_test_split çš„4ä¸ªè¿”å›å€¼
        print("ğŸ“Š æ­£åœ¨åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†...")
        X_train_split, X_val, y_train_split, y_val = train_test_split(
            X_train, y_train, test_size=0.2, shuffle=False
        )
        
        print(f"âœ… è®­ç»ƒé›†å¤§å°: {len(X_train_split):,}")
        print(f"âœ… éªŒè¯é›†å¤§å°: {len(X_val):,}")
        print(f"âœ… æ•°æ®ç±»å‹: X_train {type(X_train_split)}, y_train {type(y_train_split)}")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        print("ğŸ“¦ æ­£åœ¨åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        train_dataset = TensorDataset(
            torch.FloatTensor(X_train_split), 
            torch.FloatTensor(y_train_split)
        )
        train_loader = DataLoader(
            train_dataset, 
            batch_size=self.config['batch_size'],
            shuffle=True,
            pin_memory=True if torch.cuda.is_available() else False
        )
        
        val_dataset = TensorDataset(
            torch.FloatTensor(X_val), 
            torch.FloatTensor(y_val)
        )
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.config['batch_size'],
            shuffle=False,
            pin_memory=True if torch.cuda.is_available() else False
        )
        print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆï¼Œè®­ç»ƒæ‰¹æ¬¡æ•°é‡: {len(train_loader)}")
        
        # åˆ›å»ºæ¨¡å‹
        print("ğŸ—ï¸  æ­£åœ¨åˆ›å»ºæ¨¡å‹...")
        model = self.create_model()
        print(f"âœ… æ¨¡å‹åˆ›å»ºå®Œæˆï¼Œå‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        
        # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        print("âš™ï¸  æ­£åœ¨é…ç½®ä¼˜åŒ–å™¨...")
        criterion = nn.MSELoss()
        optimizer = optim.Adam(
            model.parameters(), 
            lr=self.config['learning_rate'],
            weight_decay=self.config.get('weight_decay', 1e-5)
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, 
            mode='min',
            factor=0.5,
            patience=10,
        )
        print(f"âœ… ä¼˜åŒ–å™¨é…ç½®å®Œæˆï¼Œå­¦ä¹ ç‡: {self.config['learning_rate']}")
        
        # è®­ç»ƒå¾ªç¯
        print("\nğŸ¯ å¼€å§‹è®­ç»ƒå¾ªç¯...")
        print("=" * 80)
        train_losses = []
        val_losses = []
        best_val_loss = float('inf')
        patience_counter = 0
        
        for epoch in range(self.config['epochs']):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0
            batch_count = 0
            
            print(f"\nğŸ“ˆ Epoch [{epoch+1:3d}/{self.config['epochs']}] å¼€å§‹è®­ç»ƒ...")
            
            for batch_X, batch_y in train_loader:
                batch_X = batch_X.to(self.device)
                batch_y = batch_y.to(self.device)
                
                optimizer.zero_grad()
                outputs = model(batch_X)
                loss = criterion(outputs.squeeze(), batch_y)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
                batch_count += 1
                
                if batch_count % 10 == 0:
                    print(f"   ğŸ“Š æ‰¹æ¬¡ {batch_count:4d}/{len(train_loader):4d}, å½“å‰æŸå¤±: {loss.item():.6f}")
            
            # éªŒè¯é˜¶æ®µ
            print(f"ğŸ” Epoch [{epoch+1:3d}] å¼€å§‹éªŒè¯...")
            model.eval()
            val_loss = 0
            val_batch_count = 0
            with torch.no_grad():
                for val_X, val_y in val_loader:
                    val_X = val_X.to(self.device)
                    val_y = val_y.to(self.device)
                    val_outputs = model(val_X)
                    val_loss += criterion(val_outputs.squeeze(), val_y).item()
                    val_batch_count += 1
                val_loss = val_loss / val_batch_count
            
            # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±
            avg_train_loss = train_loss / len(train_loader)
            train_losses.append(avg_train_loss)
            val_losses.append(val_loss)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step(val_loss)
            
            # æ—©åœæ£€æŸ¥
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹å’Œå¤„ç†å™¨
                self.save_model_and_processor(model, processor)
                print(f"   ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (éªŒè¯æŸå¤±: {val_loss:.6f})")
            else:
                patience_counter += 1
            
            # æ‰“å°è¿›åº¦
            print(f"   ğŸ“Š Epoch [{epoch+1:3d}/{self.config['epochs']}] å®Œæˆ")
            print(f"   ğŸ¯ è®­ç»ƒæŸå¤±: {avg_train_loss:.6f}")
            print(f"   ğŸ” éªŒè¯æŸå¤±: {val_loss:.6f}")
            print(f"   ğŸ“ˆ å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.6f}")
            print(f"   â° è€å¿ƒè®¡æ•°: {patience_counter}/{self.config.get('patience', 20)}")
            print("-" * 60)
            
            # æ—©åœ
            if patience_counter >= self.config.get('patience', 20):
                print(f"ğŸ›‘ æ—©åœåœ¨ç¬¬ {epoch+1} è½®")
                break
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        print("\nğŸ”„ æ­£åœ¨åŠ è½½æœ€ä½³æ¨¡å‹...")
        model.load_state_dict(torch.load("../../models/best_model.pth"))
        print(f"âœ… æœ€ä½³æ¨¡å‹åŠ è½½å®Œæˆ (éªŒè¯æŸå¤±: {best_val_loss:.6f})")
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        print("ğŸ“Š æ­£åœ¨ç»˜åˆ¶è®­ç»ƒæ›²çº¿...")
        self.plot_training_curves(train_losses, val_losses)
        print("âœ… è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ° ../../results/training_curves.png")
        
        print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print("=" * 60)
        
        return model, processor
    
    def evaluate(self, model, X_test, y_test):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print("ğŸ” æ­£åœ¨è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        print(f"ğŸ“Š æµ‹è¯•æ ·æœ¬æ•°: {len(X_test):,}")
        
        model.eval()
        with torch.no_grad():
            X_test_tensor = torch.FloatTensor(X_test).to(self.device)
            predictions = model(X_test_tensor).cpu().numpy().flatten()
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        print("ğŸ“ˆ æ­£åœ¨è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
        mse = mean_squared_error(y_test, predictions)
        mae = mean_absolute_error(y_test, predictions)
        r2 = r2_score(y_test, predictions)
        
        # è®¡ç®—æ–¹å‘å‡†ç¡®ç‡
        direction_accuracy = np.mean(
            np.sign(np.diff(y_test)) == np.sign(np.diff(predictions))
        )
        
        # æ‰“å°è¯„ä¼°ç»“æœ
        print("\nğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ:")
        print("=" * 40)
        print(f"ğŸ¯ MSE (å‡æ–¹è¯¯å·®): {mse:.6f}")
        print(f"ğŸ“ MAE (å¹³å‡ç»å¯¹è¯¯å·®): {mae:.6f}")
        print(f"ğŸ“Š RÂ² (å†³å®šç³»æ•°): {r2:.6f}")
        print(f"ğŸ“ˆ æ–¹å‘å‡†ç¡®ç‡: {direction_accuracy:.6f}")
        print("=" * 40)
        
        return {
            'mse': mse,
            'mae': mae,
            'r2': r2,
            'direction_accuracy': direction_accuracy,
            'predictions': predictions
        }
    
    def plot_training_curves(self, train_losses, val_losses):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        plt.figure(figsize=(12, 4))
        
        # ç»˜åˆ¶è®­ç»ƒå’ŒéªŒè¯æŸå¤±
        plt.subplot(1, 2, 1)
        plt.plot(train_losses, label='Training Loss')
        plt.plot(val_losses, label='Validation Loss')
        plt.title('Training and Validation Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        
        # ç»˜åˆ¶å¯¹æ•°å°ºåº¦çš„æŸå¤±æ›²çº¿
        plt.subplot(1, 2, 2)
        plt.plot(train_losses, label='Training Loss', alpha=0.7)
        plt.plot(val_losses, label='Validation Loss', alpha=0.7)
        plt.title('Training and Validation Loss (Log Scale)')
        plt.xlabel('Epoch')
        plt.ylabel('Loss (Log)')
        plt.yscale('log')
        plt.legend()
        plt.grid(True)
        
        plt.tight_layout()
        plt.savefig('../../results/training_curves.png', dpi=300, bbox_inches='tight')
        plt.show()

def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®CUDAç¯å¢ƒå˜é‡å’Œå†…å­˜ä¼˜åŒ–
    import os
    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
    
    # æ¸…ç†GPUå†…å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
    
    print("ğŸ¯ è‚¡ç¥¨ä»·æ ¼é¢„æµ‹æ¨¡å‹è®­ç»ƒç¨‹åº")
    print("=" * 60)
    
    # é…ç½®å‚æ•°
    config = {
        'seq_length': 20,
        'input_dim': 21,
        'd_model': 64,
        'nhead': 8,
        'num_layers': 2,
        'dropout': 0.1,
        'batch_size': 8,
        'learning_rate': 0.001,
        'epochs': 30,
        'patience': 10,
        'weight_decay': 1e-5,
        'model_type': 'basic',
        'target_col': 'close',
        'stock_codes': None,
        'max_samples': 50000
    }
    
    print("ğŸ“‹ é…ç½®å‚æ•°:")
    for key, value in config.items():
        print(f"   {key}: {value}")
    print()
    
    # åˆ›å»ºè®­ç»ƒå™¨
    print("ğŸ—ï¸  æ­£åœ¨åˆ›å»ºè®­ç»ƒå™¨...")
    trainer = StockTrainer(config)
    print("âœ… è®­ç»ƒå™¨åˆ›å»ºå®Œæˆ")
    
    # åŠ è½½æ•°æ®
    print("\nğŸ“Š æ­£åœ¨åŠ è½½è®­ç»ƒæ•°æ®...")
    X_train, y_train, processor = trainer.load_data()
    
    if X_train is None:
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œé€€å‡ºè®­ç»ƒ")
        return
    
    # é™åˆ¶æ•°æ®é‡ä»¥èŠ‚çœå†…å­˜
    if X_train is not None and y_train is not None and len(X_train) > config['max_samples']:
        print(f"ğŸ“Š æ•°æ®é‡è¿‡å¤§ï¼Œé™åˆ¶ä¸º {config['max_samples']:,} ä¸ªæ ·æœ¬")
        indices = np.random.choice(len(X_train), config['max_samples'], replace=False)
        X_train = X_train[indices]
        y_train = y_train[indices]
    
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œè®­ç»ƒæ ·æœ¬æ•°: {len(X_train):,}")
    
    # è®­ç»ƒæ¨¡å‹
    print("\nğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
    model, processor = trainer.train(X_train, y_train, processor)
    
    # è¯„ä¼°æ¨¡å‹ - ä¿®å¤ï¼šæ­£ç¡®æ¥æ”¶4ä¸ªè¿”å›å€¼
    print("\nğŸ“ˆ æ­£åœ¨è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
    _, X_test, _, y_test = train_test_split(X_train, y_train, test_size=0.1, shuffle=False)
    results = trainer.evaluate(model, X_test, y_test)
    
    # æ£€æŸ¥è¯„ä¼°ç»“æœ
    if results is None:
        print("âŒ æ¨¡å‹è¯„ä¼°å¤±è´¥")
        results = {'mse': 0.0, 'r2': 0.0}
    
    # æ‰“å°å®Œæˆä¿¡æ¯
    print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print("=" * 60)
    print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: ../../models/best_model.pth")
    print(f"ğŸ“Š è®­ç»ƒæ›²çº¿ä¿å­˜åœ¨: ../../results/training_curves.png")
    print(f"ğŸ“ˆ æ¨¡å‹æ€§èƒ½: MSE={results['mse']:.4f}, RÂ²={results['r2']:.4f}")
    print("=" * 60)

if __name__ == "__main__":
    main()