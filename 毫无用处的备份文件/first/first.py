"""
è‚¡ç¥¨ä»·æ ¼é¢„æµ‹æ¨¡å‹ - ä¸»è®­ç»ƒç¨‹åº
====================================

æ–‡ä»¶ä½œç”¨ï¼š
- è¿™æ˜¯æ•´ä¸ªé¡¹ç›®çš„å…¥å£æ–‡ä»¶ï¼Œè´Ÿè´£åè°ƒæ•´ä¸ªè®­ç»ƒæµç¨‹
- åŒ…å«StockTrainerç±»ï¼Œè´Ÿè´£æ¨¡å‹è®­ç»ƒã€éªŒè¯å’Œè¯„ä¼°
- å¤„ç†æ•°æ®åŠ è½½ã€æ¨¡å‹åˆ›å»ºã€è®­ç»ƒå¾ªç¯ã€æ€§èƒ½è¯„ä¼°ç­‰å®Œæ•´æµç¨‹
- æ”¯æŒGPU/CPUè®­ç»ƒï¼ŒåŒ…å«æ—©åœã€å­¦ä¹ ç‡è°ƒåº¦ç­‰è®­ç»ƒä¼˜åŒ–ç­–ç•¥

ä¸»è¦åŠŸèƒ½ï¼š
1. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
2. æ¨¡å‹åˆ›å»ºå’Œé…ç½®
3. è®­ç»ƒå¾ªç¯ç®¡ç†
4. éªŒè¯å’Œè¯„ä¼°
5. æ¨¡å‹ä¿å­˜å’Œç»“æœå¯è§†åŒ–

ä½¿ç”¨æ–¹æ³•ï¼š
python main.py

ä½œè€…ï¼šAI Assistant
åˆ›å»ºæ—¶é—´ï¼š2024å¹´
"""

# å¯¼å…¥å¿…è¦çš„åº“
import torch  # PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶
import torch.nn as nn  # ç¥ç»ç½‘ç»œæ¨¡å—
import torch.optim as optim  # ä¼˜åŒ–å™¨æ¨¡å—
from torch.utils.data import DataLoader, TensorDataset  # æ•°æ®åŠ è½½å™¨
import numpy as np  # æ•°å€¼è®¡ç®—åº“
from sklearn.model_selection import train_test_split  # æ•°æ®é›†åˆ†å‰²
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score  # è¯„ä¼°æŒ‡æ ‡
import matplotlib.pyplot as plt  # ç»˜å›¾åº“
import os  # æ“ä½œç³»ç»Ÿæ¥å£
import pandas as pd  # æ•°æ®å¤„ç†åº“
from datetime import datetime  # æ—¥æœŸæ—¶é—´å¤„ç†
import warnings  # è­¦å‘Šå¤„ç†
warnings.filterwarnings('ignore')  # å¿½ç•¥è­¦å‘Šä¿¡æ¯
import sys
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from learn.models.transformer_model import StockTransformer, AdvancedStockTransformer, create_model  # Transformeræ¨¡å‹
from data.data_processor import StockDataProcessor  # æ•°æ®å¤„ç†

class StockTrainer:
    """è‚¡ç¥¨è®­ç»ƒå™¨ç±»ï¼Œè´Ÿè´£æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°"""
    
    def __init__(self, config):
        """åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            config (dict): é…ç½®å‚æ•°å­—å…¸
        """
        self.config = config  # ä¿å­˜é…ç½®å‚æ•°
        # è®¾ç½®è®¾å¤‡ï¼ˆGPUæˆ–CPUï¼‰
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        os.makedirs('../../models', exist_ok=True)  # æ¨¡å‹ä¿å­˜ç›®å½•
        os.makedirs('../../results', exist_ok=True)  # ç»“æœä¿å­˜ç›®å½•
        os.makedirs('../../logs', exist_ok=True)  # æ—¥å¿—ä¿å­˜ç›®å½•
        
    def load_data(self):
        """åŠ è½½å’Œå¤„ç†è®­ç»ƒæ•°æ®
        
        Returns:
            tuple: (X_train, y_train, processor) è®­ç»ƒæ•°æ®å’Œæ•°æ®å¤„ç†å™¨
        """
        print("ğŸ“‚ æ­£åœ¨åŠ è½½è®­ç»ƒæ•°æ®...")
        
        # åˆ›å»ºæ•°æ®å¤„ç†å™¨å®ä¾‹
        print("ğŸ”§ æ­£åœ¨åˆ›å»ºæ•°æ®å¤„ç†å™¨...")
        processor = StockDataProcessor(seq_length=self.config['seq_length'])
        print(f"âœ… æ•°æ®å¤„ç†å™¨åˆ›å»ºå®Œæˆï¼Œåºåˆ—é•¿åº¦: {self.config['seq_length']}")
        
        # åŠ è½½è®­ç»ƒæ•°æ®ï¼ˆ2023-2024å¹´ï¼‰
        train_data_dir = "../../data/learn_csv"  # è®­ç»ƒæ•°æ®ç›®å½•
        if os.path.exists(train_data_dir):  # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
            print(f"ğŸ“ æ‰¾åˆ°è®­ç»ƒæ•°æ®ç›®å½•: {train_data_dir}")
            # åŠ è½½å¤šä¸ªè‚¡ç¥¨çš„æ•°æ®
            print("ğŸ“Š æ­£åœ¨åŠ è½½è‚¡ç¥¨æ•°æ®...")
            train_stock_data = processor.load_multiple_stocks(
                train_data_dir, 
                stock_codes=self.config.get('stock_codes', None)  # æŒ‡å®šè‚¡ç¥¨ä»£ç ï¼ŒNoneè¡¨ç¤ºåŠ è½½æ‰€æœ‰
            )
            print(f"âœ… æˆåŠŸåŠ è½½äº† {len(train_stock_data)} åªè‚¡ç¥¨çš„è®­ç»ƒæ•°æ®")
        else:
            print(f"âŒ è®­ç»ƒæ•°æ®ç›®å½•ä¸å­˜åœ¨: {train_data_dir}")
            return None, None, None  # è¿”å›ä¸‰ä¸ªNoneå€¼
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        print("ğŸ”„ æ­£åœ¨å‡†å¤‡è®­ç»ƒæ•°æ®...")
        X_train, y_train = processor.prepare_multi_stock_data(
            train_stock_data, 
            target_col=self.config['target_col']  # é¢„æµ‹ç›®æ ‡åˆ—ï¼ˆé€šå¸¸æ˜¯æ”¶ç›˜ä»·ï¼‰
        )
        
        if X_train is None:  # æ£€æŸ¥æ•°æ®å‡†å¤‡æ˜¯å¦æˆåŠŸ
            print("âŒ è®­ç»ƒæ•°æ®å‡†å¤‡å¤±è´¥")
            return None, None, None
        
        print(f"âœ… è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæˆ")
        if X_train is not None and y_train is not None:
            print(f"   ğŸ“Š ç‰¹å¾æ•°æ®å½¢çŠ¶: {X_train.shape if hasattr(X_train, 'shape') else 'unknown'}")
            print(f"   ğŸ“Š æ ‡ç­¾æ•°æ®å½¢çŠ¶: {y_train.shape if hasattr(y_train, 'shape') else 'unknown'}")
        else:
            print("   ğŸ“Š æ•°æ®å½¢çŠ¶: æ•°æ®ä¸ºNone")
            
        return X_train, y_train, processor  # è¿”å›è®­ç»ƒæ•°æ®å’Œå¤„ç†å™¨
    
    def create_model(self):
        """åˆ›å»ºTransformeræ¨¡å‹
        
        Returns:
            nn.Module: åˆ›å»ºçš„æ¨¡å‹å®ä¾‹
        """
        model_type = self.config.get('model_type', 'basic')  # è·å–æ¨¡å‹ç±»å‹
        
        if model_type == 'basic':  # åŸºæœ¬Transformeræ¨¡å‹
            model = StockTransformer(
                input_dim=self.config['input_dim'],  # è¾“å…¥ç‰¹å¾ç»´åº¦
                d_model=self.config['d_model'],  # æ¨¡å‹ç»´åº¦
                nhead=self.config['nhead'],  # æ³¨æ„åŠ›å¤´æ•°
                num_layers=self.config['num_layers'],  # Transformerå±‚æ•°
                seq_len=self.config['seq_length'],  # åºåˆ—é•¿åº¦
                output_dim=1,  # è¾“å‡ºç»´åº¦ï¼ˆé¢„æµ‹ä¸€ä¸ªå€¼ï¼‰
                dropout=self.config['dropout']  # Dropoutç‡
            )
        elif model_type == 'advanced':  # é«˜çº§Transformeræ¨¡å‹
            model = AdvancedStockTransformer(
                input_dim=self.config['input_dim'],
                d_model=self.config['d_model'],
                nhead=self.config['nhead'],
                num_layers=self.config['num_layers'],
                seq_len=self.config['seq_length'],
                output_dim=1,
                dropout=self.config['dropout']
            )
        else:  # å…¶ä»–æ¨¡å‹ç±»å‹
            model = create_model(model_type, **self.config)
        
        return model.to(self.device)  # å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
    
    def train(self, X_train, y_train, processor):
        """è®­ç»ƒæ¨¡å‹
        
        Args:
            X_train (np.ndarray): è®­ç»ƒç‰¹å¾æ•°æ®
            y_train (np.ndarray): è®­ç»ƒæ ‡ç­¾æ•°æ®
            processor (StockDataProcessor): æ•°æ®å¤„ç†å™¨
            
        Returns:
            tuple: (model, processor) è®­ç»ƒå¥½çš„æ¨¡å‹å’Œæ•°æ®å¤„ç†å™¨
        """
        print("ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        print("=" * 60)
        
        # å†…å­˜ä¼˜åŒ–
        torch.cuda.empty_cache()
        torch.backends.cudnn.benchmark = True
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆæ—¶é—´åºåˆ—ä¸éšæœºæ‰“ä¹±ï¼‰
        print("ğŸ“Š æ­£åœ¨åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†...")
        X_train_split, X_val, y_train_split, y_val = train_test_split(
            X_train, y_train, test_size=0.2, shuffle=False
        )
        
        # æ‰“å°æ•°æ®é›†å¤§å°
        print(f"âœ… è®­ç»ƒé›†å¤§å°: {len(X_train_split):,}")
        print(f"âœ… éªŒè¯é›†å¤§å°: {len(X_val):,}")
        # æ˜¾ç¤ºæ•°æ®ç±»å‹
        print(f"âœ… æ•°æ®ç±»å‹: X_train {type(X_train_split)}, y_train {type(y_train_split)}")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆä¸é¢„å…ˆç§»åŠ¨åˆ°GPUä»¥èŠ‚çœå†…å­˜ï¼‰
        print("ğŸ“¦ æ­£åœ¨åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        train_dataset = TensorDataset(
            torch.FloatTensor(X_train_split), 
            torch.FloatTensor(y_train_split)
        )
        train_loader = DataLoader(
            train_dataset, 
            batch_size=self.config['batch_size'],  # æ‰¹æ¬¡å¤§å°
            shuffle=True,  # éšæœºæ‰“ä¹±
            pin_memory=True if torch.cuda.is_available() else False  # å†…å­˜ä¼˜åŒ–
        )
        
        # éªŒè¯é›†æ•°æ®ï¼ˆå°æ‰¹é‡å¤„ç†ä»¥èŠ‚çœå†…å­˜ï¼‰
        val_dataset = TensorDataset(
            torch.FloatTensor(X_val), 
            torch.FloatTensor(y_val)
        )
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.config['batch_size'],
            shuffle=False,
            pin_memory=True if torch.cuda.is_available() else False
        )
        print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆï¼Œè®­ç»ƒæ‰¹æ¬¡æ•°é‡: {len(train_loader)}")
        
        # åˆ›å»ºæ¨¡å‹
        print("ğŸ—ï¸  æ­£åœ¨åˆ›å»ºæ¨¡å‹...")
        model = self.create_model()
        print(f"âœ… æ¨¡å‹åˆ›å»ºå®Œæˆï¼Œå‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        
        # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        print("âš™ï¸  æ­£åœ¨é…ç½®ä¼˜åŒ–å™¨...")
        criterion = nn.MSELoss()  # å‡æ–¹è¯¯å·®æŸå¤±
        optimizer = optim.Adam(
            model.parameters(), 
            lr=self.config['learning_rate'],  # å­¦ä¹ ç‡
            weight_decay=self.config.get('weight_decay', 1e-5)  # æƒé‡è¡°å‡
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå½“éªŒè¯æŸå¤±ä¸ä¸‹é™æ—¶é™ä½å­¦ä¹ ç‡ï¼‰
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, 
            mode='min',  # ç›‘æ§æœ€å°å€¼
            factor=0.5,  # å­¦ä¹ ç‡è¡°å‡å› å­
            patience=10,  # è€å¿ƒå€¼
        )
        print(f"âœ… ä¼˜åŒ–å™¨é…ç½®å®Œæˆï¼Œå­¦ä¹ ç‡: {self.config['learning_rate']}")
        
        # è®­ç»ƒå¾ªç¯
        print("\nğŸ¯ å¼€å§‹è®­ç»ƒå¾ªç¯...")
        print("=" * 80)
        train_losses = []  # è®­ç»ƒæŸå¤±è®°å½•
        val_losses = []  # éªŒè¯æŸå¤±è®°å½•
        best_val_loss = float('inf')  # æœ€ä½³éªŒè¯æŸå¤±
        patience_counter = 0  # æ—©åœè®¡æ•°å™¨
        
        for epoch in range(self.config['epochs']):  # è®­ç»ƒè½®æ•°
            # è®­ç»ƒé˜¶æ®µ
            model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
            train_loss = 0  # åˆå§‹åŒ–è®­ç»ƒæŸå¤±
            batch_count = 0  # æ‰¹æ¬¡è®¡æ•°å™¨
            
            # æ˜¾ç¤ºè¿›åº¦æ¡
            print(f"\nğŸ“ˆ Epoch [{epoch+1:3d}/{self.config['epochs']}] å¼€å§‹è®­ç»ƒ...")
            
            for batch_X, batch_y in train_loader:  # éå†æ¯ä¸ªæ‰¹æ¬¡
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                batch_X = batch_X.to(self.device)
                batch_y = batch_y.to(self.device)
                
                optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦
                outputs = model(batch_X)  # å‰å‘ä¼ æ’­
                loss = criterion(outputs.squeeze(), batch_y)  # è®¡ç®—æŸå¤±
                loss.backward()  # åå‘ä¼ æ’­
                
                # æ¢¯åº¦è£å‰ª (å¯é€‰) - é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
                # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                optimizer.step()  # æ›´æ–°å‚æ•°
                train_loss += loss.item()  # ç´¯åŠ æŸå¤±
                batch_count += 1
                
                # æ¯10ä¸ªæ‰¹æ¬¡æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                if batch_count % 10 == 0:
                    print(f"   ğŸ“Š æ‰¹æ¬¡ {batch_count:4d}/{len(train_loader):4d}, å½“å‰æŸå¤±: {loss.item():.6f}")
            
            # éªŒè¯é˜¶æ®µ
            print(f"ğŸ” Epoch [{epoch+1:3d}] å¼€å§‹éªŒè¯...")
            model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            val_loss = 0
            val_batch_count = 0
            with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦
                for val_X, val_y in val_loader:  # å°æ‰¹é‡éªŒè¯
                    val_X = val_X.to(self.device)
                    val_y = val_y.to(self.device)
                    val_outputs = model(val_X)  # éªŒè¯é›†é¢„æµ‹
                    val_loss += criterion(val_outputs.squeeze(), val_y).item()  # éªŒè¯æŸå¤±
                    val_batch_count += 1
                val_loss = val_loss / val_batch_count  # å¹³å‡éªŒè¯æŸå¤±
            
            # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±
            avg_train_loss = train_loss / len(train_loader)
            train_losses.append(avg_train_loss)  # è®°å½•è®­ç»ƒæŸå¤±
            val_losses.append(val_loss)  # è®°å½•éªŒè¯æŸå¤±ï¼ˆå·²ç»æ˜¯floatï¼‰
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step(val_loss)
            
            # æ—©åœæ£€æŸ¥
            if val_loss < best_val_loss:  # å¦‚æœéªŒè¯æŸå¤±æ›´å°
                best_val_loss = val_loss  # æ›´æ–°æœ€ä½³éªŒè¯æŸå¤±
                patience_counter = 0  # é‡ç½®è®¡æ•°å™¨
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                torch.save(model.state_dict(), f"../../models/best_model.pth")
                print(f"   ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (éªŒè¯æŸå¤±: {val_loss:.6f})")
            else:
                patience_counter += 1  # å¢åŠ è®¡æ•°å™¨
            
            # æ‰“å°è¿›åº¦
            print(f"   ğŸ“Š Epoch [{epoch+1:3d}/{self.config['epochs']}] å®Œæˆ")
            print(f"   ğŸ¯ è®­ç»ƒæŸå¤±: {avg_train_loss:.6f}")
            print(f"   ğŸ” éªŒè¯æŸå¤±: {val_loss:.6f}")
            print(f"   ğŸ“ˆ å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.6f}")
            print(f"   â° è€å¿ƒè®¡æ•°: {patience_counter}/{self.config.get('patience', 20)}")
            print("-" * 60)
            
            # æ—©åœ
            if patience_counter >= self.config.get('patience', 20):  # å¦‚æœè¶…è¿‡è€å¿ƒå€¼
                print(f"ğŸ›‘ æ—©åœåœ¨ç¬¬ {epoch+1} è½®")
                break
            
            # æ—©åœæ£€æŸ¥
            if val_loss < best_val_loss:  # å¦‚æœéªŒè¯æŸå¤±æ›´å°
                best_val_loss = val_loss  # æ›´æ–°æœ€ä½³éªŒè¯æŸå¤±
                patience_counter = 0  # é‡ç½®è®¡æ•°å™¨
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                torch.save(model.state_dict(), f"../../models/best_model.pth")
            else:
                patience_counter += 1  # å¢åŠ è®¡æ•°å™¨
            
            # æ‰“å°è¿›åº¦
            if (epoch + 1) % 10 == 0:  # æ¯10è½®æ‰“å°ä¸€æ¬¡
                print(f'Epoch [{epoch+1}/{self.config["epochs"]}], '
                      f'Train Loss: {avg_train_loss:.4f}, '
                      f'Val Loss: {val_loss:.4f}, '
                      f'LR: {optimizer.param_groups[0]["lr"]:.6f}')
            
            # æ—©åœ
            if patience_counter >= self.config.get('patience', 20):  # å¦‚æœè¶…è¿‡è€å¿ƒå€¼
                print(f"æ—©åœåœ¨ç¬¬ {epoch+1} è½®")
                break
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        print("\nğŸ”„ æ­£åœ¨åŠ è½½æœ€ä½³æ¨¡å‹...")
        model.load_state_dict(torch.load("../../models/best_model.pth"))
        print(f"âœ… æœ€ä½³æ¨¡å‹åŠ è½½å®Œæˆ (éªŒè¯æŸå¤±: {best_val_loss:.6f})")
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        print("ğŸ“Š æ­£åœ¨ç»˜åˆ¶è®­ç»ƒæ›²çº¿...")
        self.plot_training_curves(train_losses, val_losses)
        print("âœ… è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ° ../../results/training_curves.png")
        
        print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print("=" * 60)
        
        return model, processor
    
    def evaluate(self, model, X_test, y_test):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½
        
        Args:
            model (nn.Module): è®­ç»ƒå¥½çš„æ¨¡å‹
            X_test (np.ndarray): æµ‹è¯•ç‰¹å¾æ•°æ®
            y_test (np.ndarray): æµ‹è¯•æ ‡ç­¾æ•°æ®
            
        Returns:
            dict: è¯„ä¼°ç»“æœå­—å…¸
        """
        print("ğŸ” æ­£åœ¨è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        print(f"ğŸ“Š æµ‹è¯•æ ·æœ¬æ•°: {len(X_test):,}")
        
        model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦
            X_test_tensor = torch.FloatTensor(X_test).to(self.device)  # è½¬æ¢ä¸ºå¼ é‡
            predictions = model(X_test_tensor).cpu().numpy().flatten()  # è·å–é¢„æµ‹ç»“æœ
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        print("ğŸ“ˆ æ­£åœ¨è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
        mse = mean_squared_error(y_test, predictions)  # å‡æ–¹è¯¯å·®
        mae = mean_absolute_error(y_test, predictions)  # å¹³å‡ç»å¯¹è¯¯å·®
        r2 = r2_score(y_test, predictions)  # å†³å®šç³»æ•°
        
        # è®¡ç®—æ–¹å‘å‡†ç¡®ç‡ï¼ˆé¢„æµ‹ä»·æ ¼å˜åŠ¨æ–¹å‘çš„å‡†ç¡®ç‡ï¼‰
        direction_accuracy = np.mean(
            np.sign(np.diff(y_test)) == np.sign(np.diff(predictions))
        )
        
        # æ‰“å°è¯„ä¼°ç»“æœ
        print("\nğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ:")
        print("=" * 40)
        print(f"ğŸ¯ MSE (å‡æ–¹è¯¯å·®): {mse:.6f}")
        print(f"ğŸ“ MAE (å¹³å‡ç»å¯¹è¯¯å·®): {mae:.6f}")
        print(f"ğŸ“Š RÂ² (å†³å®šç³»æ•°): {r2:.6f}")
        print(f"ğŸ“ˆ æ–¹å‘å‡†ç¡®ç‡: {direction_accuracy:.6f}")
        print("=" * 40)
        
        return {
            'mse': mse,
            'mae': mae,
            'r2': r2,
            'direction_accuracy': direction_accuracy,
            'predictions': predictions
        }
    
    def plot_training_curves(self, train_losses, val_losses):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        
        Args:
            train_losses (list): è®­ç»ƒæŸå¤±åˆ—è¡¨
            val_losses (list): éªŒè¯æŸå¤±åˆ—è¡¨
        """
        plt.figure(figsize=(12, 4))  # åˆ›å»ºå›¾å½¢
        
        # ç»˜åˆ¶è®­ç»ƒå’ŒéªŒè¯æŸå¤±
        plt.subplot(1, 2, 1)
        plt.plot(train_losses, label='Training Loss')  # è®­ç»ƒæŸå¤±
        plt.plot(val_losses, label='Validation Loss')  # éªŒè¯æŸå¤±
        plt.title('Training and Validation Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        
        # ç»˜åˆ¶å¯¹æ•°å°ºåº¦çš„æŸå¤±æ›²çº¿
        plt.subplot(1, 2, 2)
        plt.plot(train_losses, label='Training Loss', alpha=0.7)
        plt.plot(val_losses, label='Validation Loss', alpha=0.7)
        plt.title('Training and Validation Loss (Log Scale)')
        plt.xlabel('Epoch')
        plt.ylabel('Loss (Log)')
        plt.yscale('log')  # å¯¹æ•°å°ºåº¦
        plt.legend()
        plt.grid(True)
        
        plt.tight_layout()  # è°ƒæ•´å¸ƒå±€
        plt.savefig('../../results/training_curves.png', dpi=300, bbox_inches='tight')  # ä¿å­˜å›¾ç‰‡
        plt.show()  # æ˜¾ç¤ºå›¾ç‰‡
    
    def predict_next_price(self, model, processor, stock_code):
        """é¢„æµ‹ä¸‹ä¸€ä¸ªä»·æ ¼
        
        Args:
            model (nn.Module): è®­ç»ƒå¥½çš„æ¨¡å‹
            processor (StockDataProcessor): æ•°æ®å¤„ç†å™¨
            stock_code (str): è‚¡ç¥¨ä»£ç 
            
        Returns:
            float: é¢„æµ‹çš„ä¸‹ä¸€ä¸ªä»·æ ¼
        """
        # åŠ è½½æµ‹è¯•æ•°æ®ï¼ˆ2025å¹´ï¼‰
        test_data_dir = "../../data/test_csv"
        if not os.path.exists(test_data_dir):
            print(f"æµ‹è¯•æ•°æ®ç›®å½•ä¸å­˜åœ¨: {test_data_dir}")
            return None
        
        # åŠ è½½ç‰¹å®šè‚¡ç¥¨çš„æœ€æ–°æ•°æ®
        csv_path = os.path.join(test_data_dir, f"{stock_code}.csv")
        if not os.path.exists(csv_path):
            print(f"è‚¡ç¥¨ {stock_code} çš„æµ‹è¯•æ•°æ®ä¸å­˜åœ¨")
            return None
        
        # åŠ è½½æ•°æ®
        latest_data = processor.load_csv_data(csv_path)
        if latest_data is None:
            return None
        
        # æ·»åŠ æŠ€æœ¯æŒ‡æ ‡
        latest_data = processor.add_technical_indicators(latest_data)
        
        # å‡†å¤‡ç‰¹å¾åˆ—
        feature_columns = [
            'open', 'high', 'low', 'close', 'vol', 'amount',
            'sma_5', 'sma_10', 'sma_20', 'ema_12', 'ema_26',
            'macd', 'macd_signal', 'macd_histogram', 'rsi',
            'bb_upper', 'bb_lower', 'bb_middle', 'volume_ratio',
            'price_position', 'volatility'
        ]
        
        # æ£€æŸ¥å“ªäº›ç‰¹å¾åˆ—å­˜åœ¨
        available_features = [col for col in feature_columns if col in latest_data.columns]
        
        if len(available_features) != self.config['input_dim']:
            print(f"ç‰¹å¾ç»´åº¦ä¸åŒ¹é…: æœŸæœ› {self.config['input_dim']}, å®é™… {len(available_features)}")
            return None
        
        # è·å–æ•°æ®å€¼
        data_values = latest_data[available_features].dropna().values
        
        if len(data_values) < processor.seq_length:
            print("æ•°æ®ä¸è¶³ï¼Œæ— æ³•é¢„æµ‹")
            return None
        
        # æ ‡å‡†åŒ–æ•°æ®
        data_scaled = processor.scaler.transform(data_values)
        
        # è·å–æœ€åä¸€ä¸ªåºåˆ—
        last_sequence = data_scaled[-processor.seq_length:]
        
        # è½¬æ¢ä¸ºå¼ é‡
        X_pred = torch.FloatTensor(last_sequence).unsqueeze(0).to(self.device)
        
        # é¢„æµ‹
        model.eval()
        with torch.no_grad():
            prediction = model(X_pred).cpu().numpy()[0, 0]
        
        return prediction

def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®CUDAç¯å¢ƒå˜é‡å’Œå†…å­˜ä¼˜åŒ–
    import os
    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # åŒæ­¥CUDAæ‰§è¡Œï¼Œä¾¿äºè°ƒè¯•
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # å†…å­˜åˆ†é…ä¼˜åŒ–
    
    # æ¸…ç†GPUå†…å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
    
    print("ğŸ¯ è‚¡ç¥¨ä»·æ ¼é¢„æµ‹æ¨¡å‹è®­ç»ƒç¨‹åº")
    print("=" * 60)
    
    # é…ç½®å‚æ•° - å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬
    config = {
        'seq_length': 20,           # å‡å°‘åºåˆ—é•¿åº¦ï¼Œé™ä½å†…å­˜ä½¿ç”¨
        'input_dim': 21,            # ä¿æŒç‰¹å¾ç»´åº¦
        'd_model': 64,              # å‡å°‘æ¨¡å‹ç»´åº¦ï¼š64/8=8
        'nhead': 8,                 # 8ä¸ªå¤´ï¼Œ64/8=8
        'num_layers': 2,            # è¿›ä¸€æ­¥å‡å°‘å±‚æ•°
        'dropout': 0.1,             # ä¿æŒ
        'batch_size': 8,            # å¤§å¹…å‡å°‘æ‰¹æ¬¡å¤§å°
        'learning_rate': 0.001,     # ä¿æŒ
        'epochs': 30,               # å‡å°‘è½®æ•°
        'patience': 10,             # æ—©åœè€å¿ƒå€¼
        'weight_decay': 1e-5,       # ä¿æŒ
        'model_type': 'basic',      # åŸºç¡€æ¨¡å‹
        'target_col': 'close',      # é¢„æµ‹æ”¶ç›˜ä»·
        'stock_codes': None,        # ä½¿ç”¨å…¨éƒ¨è‚¡ç¥¨
        'max_samples': 50000        # é™åˆ¶æœ€å¤§æ ·æœ¬æ•°
    }
    
    print("ğŸ“‹ é…ç½®å‚æ•°:")
    for key, value in config.items():
        print(f"   {key}: {value}")
    print()
    
    # åˆ›å»ºè®­ç»ƒå™¨
    print("ğŸ—ï¸  æ­£åœ¨åˆ›å»ºè®­ç»ƒå™¨...")
    trainer = StockTrainer(config)
    print("âœ… è®­ç»ƒå™¨åˆ›å»ºå®Œæˆ")
    
    # åŠ è½½æ•°æ®
    print("\nğŸ“Š æ­£åœ¨åŠ è½½è®­ç»ƒæ•°æ®...")
    X_train, y_train, processor = trainer.load_data()
    
    if X_train is None:  # æ£€æŸ¥æ•°æ®åŠ è½½æ˜¯å¦æˆåŠŸ
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œé€€å‡ºè®­ç»ƒ")
        return
    
    # é™åˆ¶æ•°æ®é‡ä»¥èŠ‚çœå†…å­˜
    if X_train is not None and y_train is not None and len(X_train) > config['max_samples']:
        print(f"ğŸ“Š æ•°æ®é‡è¿‡å¤§ï¼Œé™åˆ¶ä¸º {config['max_samples']:,} ä¸ªæ ·æœ¬")
        indices = np.random.choice(len(X_train), config['max_samples'], replace=False)
        X_train = X_train[indices]
        y_train = y_train[indices]
    
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œè®­ç»ƒæ ·æœ¬æ•°: {len(X_train):,}")
    
    # è®­ç»ƒæ¨¡å‹
    print("\nğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
    model, processor = trainer.train(X_train, y_train, processor)
    
    # è¯„ä¼°æ¨¡å‹ï¼ˆä½¿ç”¨éƒ¨åˆ†è®­ç»ƒæ•°æ®ä½œä¸ºæµ‹è¯•é›†ï¼‰
    print("\nğŸ“ˆ æ­£åœ¨è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
    X_test, y_test = train_test_split(X_train, y_train, test_size=0.1, shuffle=False)
    results = trainer.evaluate(model, X_test, y_test)
    
    # æ£€æŸ¥è¯„ä¼°ç»“æœ
    if results is None:
        print("âŒ æ¨¡å‹è¯„ä¼°å¤±è´¥")
        results = {'mse': 0.0, 'r2': 0.0}  # é»˜è®¤å€¼
    
    # é¢„æµ‹ç¤ºä¾‹
    print("\nğŸ”® æ­£åœ¨è¿›è¡Œé¢„æµ‹ç¤ºä¾‹...")
    stock_codes = config.get('stock_codes', None)
    if stock_codes and len(stock_codes) > 0:  # å¦‚æœæŒ‡å®šäº†è‚¡ç¥¨ä»£ç 
        stock_code = stock_codes[0]
        print(f"ğŸ“Š é¢„æµ‹è‚¡ç¥¨ {stock_code} çš„ä¸‹ä¸€ä¸ªä»·æ ¼...")
        next_price = trainer.predict_next_price(model, processor, stock_code)
        if next_price:
            print(f"ğŸ¯ é¢„æµ‹çš„ä¸‹ä¸€ä¸ªæ”¶ç›˜ä»·: {next_price:.2f}")
    else:
        # è·å–ç¬¬ä¸€ä¸ªå¯ç”¨çš„è‚¡ç¥¨ä»£ç 
        test_data_dir = "../../data/test_csv"
        if os.path.exists(test_data_dir):
            csv_files = [f for f in os.listdir(test_data_dir) if f.endswith('.csv')]
            if csv_files:
                stock_code = csv_files[0].replace('.csv', '')
                print(f"ğŸ“Š é¢„æµ‹è‚¡ç¥¨ {stock_code} çš„ä¸‹ä¸€ä¸ªä»·æ ¼...")
                next_price = trainer.predict_next_price(model, processor, stock_code)
                if next_price:
                    print(f"ğŸ¯ é¢„æµ‹çš„ä¸‹ä¸€ä¸ªæ”¶ç›˜ä»·: {next_price:.2f}")
    
    # æ‰“å°å®Œæˆä¿¡æ¯
    print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print("=" * 60)
    print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: ../../models/best_model.pth")
    print(f"ğŸ“Š è®­ç»ƒæ›²çº¿ä¿å­˜åœ¨: ../../results/training_curves.png")
    print(f"ğŸ“ˆ æ¨¡å‹æ€§èƒ½: MSE={results['mse']:.4f}, RÂ²={results['r2']:.4f}")
    print("=" * 60)

if __name__ == "__main__":
    main()  # è¿è¡Œä¸»å‡½æ•° 