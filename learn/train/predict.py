import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
import math
from tqdm import tqdm
import json
from collections import defaultdict
import glob

warnings.filterwarnings('ignore')

class StockDataset(Dataset):
    """è‚¡ç¥¨æ•°æ®é›†ç±»"""
    def __init__(self, data, sequence_length=30, target_column='close'):
        self.data = data
        self.sequence_length = sequence_length
        self.target_column = target_column
        self.feature_columns = ['open', 'high', 'low', 'close', 'vol', 'amount']
        
        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡æ•°æ®
        self.prepare_data()
    
    def prepare_data(self):
        """å‡†å¤‡åºåˆ—æ•°æ®"""
        self.sequences = []
        self.targets = []
        self.metadata = []  # å­˜å‚¨å…ƒæ•°æ®ï¼ˆè‚¡ç¥¨ä»£ç ã€æ—¥æœŸç­‰ï¼‰
        
        # æŒ‰è‚¡ç¥¨ä»£ç åˆ†ç»„
        for secucode in self.data['secucode'].unique():
            stock_data = self.data[self.data['secucode'] == secucode].sort_values('tradingday')
            
            if len(stock_data) < self.sequence_length + 1:
                continue
                
            # è·å–ç‰¹å¾æ•°æ®
            features = stock_data[self.feature_columns].values
            
            # åˆ›å»ºåºåˆ—
            for i in range(len(features) - self.sequence_length):
                seq = features[i:i+self.sequence_length]
                target = features[i+self.sequence_length][self.feature_columns.index(self.target_column)]
                
                self.sequences.append(seq)
                self.targets.append(target)
                
                # å­˜å‚¨å…ƒæ•°æ®
                self.metadata.append({
                    'secucode': secucode,
                    'date': stock_data.iloc[i+self.sequence_length]['tradingday']
                })
        
        self.sequences = np.array(self.sequences, dtype=np.float32)
        self.targets = np.array(self.targets, dtype=np.float32)
    
    def __len__(self):
        return len(self.sequences)
    
    def __getitem__(self, idx):
        return torch.tensor(self.sequences[idx]), torch.tensor(self.targets[idx])

class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç """
    def __init__(self, d_model, max_len=1000):
        super(PositionalEncoding, self).__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        return x + self.pe[:x.size(0), :]

class TransformerStockPredictor(nn.Module):
    """Transformerè‚¡ç¥¨é¢„æµ‹æ¨¡å‹"""
    def __init__(self, input_dim=6, d_model=128, nhead=8, num_layers=4, 
                 dim_feedforward=512, sequence_length=30, dropout=0.1):
        super(TransformerStockPredictor, self).__init__()
        
        self.input_dim = input_dim
        self.d_model = d_model
        self.sequence_length = sequence_length
        
        # è¾“å…¥æŠ•å½±å±‚
        self.input_projection = nn.Linear(input_dim, d_model)
        
        # ä½ç½®ç¼–ç 
        self.positional_encoding = PositionalEncoding(d_model, sequence_length)
        
        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            activation='relu'
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # è¾“å‡ºå±‚
        self.output_projection = nn.Sequential(
            nn.Linear(d_model, dim_feedforward // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(dim_feedforward // 2, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, 1)
        )
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        # x shape: (batch_size, sequence_length, input_dim)
        batch_size, seq_len, _ = x.shape
        
        # æŠ•å½±åˆ°æ¨¡å‹ç»´åº¦
        x = self.input_projection(x)  # (batch_size, sequence_length, d_model)
        
        # è½¬ç½®ä»¥é€‚åº”Transformerè¾“å…¥æ ¼å¼
        x = x.transpose(0, 1)  # (sequence_length, batch_size, d_model)
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        x = self.positional_encoding(x)
        x = self.dropout(x)
        
        # Transformerç¼–ç 
        transformer_output = self.transformer_encoder(x)
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        last_output = transformer_output[-1]  # (batch_size, d_model)
        
        # é¢„æµ‹è¾“å‡º
        output = self.output_projection(last_output)
        
        return output.squeeze(-1)

class StockPredictor:
    """è‚¡ç¥¨é¢„æµ‹å™¨ä¸»ç±»"""
    def __init__(self, model_save_dir='models', results_dir='results'):
        self.model_save_dir = model_save_dir
        self.results_dir = results_dir
        
        # åˆ›å»ºç»“æœç›®å½•
        os.makedirs(self.results_dir, exist_ok=True)
        
        # è®¾å¤‡é€‰æ‹©
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # æ•°æ®æ ‡å‡†åŒ–å™¨
        self.scaler = None
        self.model = None
    def load_test_data(self, test_data_dir='./data/test_csv'):
        """åŠ è½½æµ‹è¯•æ•°æ®ï¼ˆ1-5æœˆä»½æ•°æ®ï¼‰"""
        print("åŠ è½½æµ‹è¯•æ•°æ®...")
        
        # æ·»åŠ ç›®å½•å­˜åœ¨æ€§æ£€æŸ¥
        if not os.path.exists(test_data_dir):
            raise FileNotFoundError(f"æµ‹è¯•æ•°æ®ç›®å½•ä¸å­˜åœ¨: {test_data_dir}")
        
        all_data = []
        csv_files = glob.glob(os.path.join(test_data_dir, '*.csv'))
        print(f"æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
        
        for file in tqdm(csv_files, desc="åŠ è½½æµ‹è¯•CSVæ–‡ä»¶"):
            try:
                # æ·»åŠ ç¼–ç å°è¯•
                try:
                    df = pd.read_csv(file, dtype={'tradingday': str})
                except UnicodeDecodeError:
                    df = pd.read_csv(file, encoding='gbk', dtype={'tradingday': str})  # å°è¯•ä¸­æ–‡ç¼–ç 
                    
                if not df.empty:
                    print(f"åŠ è½½æ–‡ä»¶ {file} æˆåŠŸï¼Œè®°å½•æ•°: {len(df)}")
                    all_data.append(df)
                else:
                    print(f"è­¦å‘Š: æ–‡ä»¶ {file} ä¸ºç©º")
            except Exception as e:
                print(f"åŠ è½½æ–‡ä»¶ {file} æ—¶å‡ºé”™: {e}")
        
        if not all_data:
            raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„æµ‹è¯•æ•°æ®æ–‡ä»¶")
        
        # åˆå¹¶æ—¶æ˜¾ç¤ºè¿›åº¦
        print("åˆå¹¶æ•°æ®...")
        combined_data = pd.concat(all_data, ignore_index=True)
        print(f"åˆå¹¶åæ€»è®°å½•æ•°: {len(combined_data)}")
        
        # é¢„å¤„ç†å‰è®°å½•æ•°
        print("é¢„å¤„ç†å‰è®°å½•æ•°:", len(combined_data))
        combined_data = self.preprocess_data(combined_data)
        print("é¢„å¤„ç†åè®°å½•æ•°:", len(combined_data))
        
        return combined_data
    
    def load_actual_data(self, actual_data_dir='./data/Adjustment_csv'):
        """åŠ è½½6æœˆä»½å®é™…æ•°æ®"""
        print("åŠ è½½6æœˆä»½å®é™…æ•°æ®...")
        
        if not os.path.exists(actual_data_dir):
            raise FileNotFoundError(f"å®é™…æ•°æ®ç›®å½•ä¸å­˜åœ¨: {actual_data_dir}")
        
        all_data = []
        csv_files = glob.glob(os.path.join(actual_data_dir, '*.csv'))
        print(f"æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
        
        for file in tqdm(csv_files, desc="åŠ è½½å®é™…æ•°æ®CSVæ–‡ä»¶"):
            try:
                # å°è¯•å¤šç§ç¼–ç 
                for encoding in ['utf-8', 'gbk', 'latin1']:
                    try:
                        df = pd.read_csv(file, encoding=encoding, dtype={'tradingday': str})
                        break
                    except UnicodeDecodeError:
                        continue
                
                if df.empty:
                    print(f"è­¦å‘Š: æ–‡ä»¶ {file} ä¸ºç©º")
                    continue
                    
                # æ£€æŸ¥æ—¥æœŸåˆ—æ˜¯å¦å­˜åœ¨
                date_col = None
                for col in ['tradingday', 'date', 'datetime', 'äº¤æ˜“æ—¥æœŸ']:
                    if col in df.columns:
                        date_col = col
                        break
                
                if not date_col:
                    print(f"è­¦å‘Š: æ–‡ä»¶ {file} æ— æ—¥æœŸåˆ—")
                    continue
                    
                # è½¬æ¢æ—¥æœŸæ ¼å¼
                try:
                    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
                    # ç§»é™¤æ— æ•ˆæ—¥æœŸ
                    df = df[df[date_col].notna()]
                except Exception as e:
                    print(f"æ–‡ä»¶ {file} æ—¥æœŸè½¬æ¢å¤±è´¥: {e}")
                    continue
                    
                all_data.append(df)
                
            except Exception as e:
                print(f"åŠ è½½æ–‡ä»¶ {file} æ—¶å‡ºé”™: {e}")
        
        if not all_data:
            raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„å®é™…æ•°æ®æ–‡ä»¶")
        
        combined_data = pd.concat(all_data, ignore_index=True)
        print(f"åˆå¹¶åæ€»è®°å½•æ•°: {len(combined_data)}")
        
        # æŸ¥æ‰¾æ—¥æœŸåˆ—
        date_col = None
        for col in ['tradingday', 'date', 'datetime', 'äº¤æ˜“æ—¥æœŸ']:
            if col in combined_data.columns:
                date_col = col
                break
        
        if not date_col:
            raise ValueError("æ•°æ®ä¸­æœªæ‰¾åˆ°æ—¥æœŸåˆ—")
        
        # æ‰“å°æ—¥æœŸèŒƒå›´
        print("æ•°æ®æ—¥æœŸèŒƒå›´:", combined_data[date_col].min(), "è‡³", combined_data[date_col].max())
        
        # ç­›é€‰6æœˆæ•°æ®ï¼ˆæ‰€æœ‰å¹´ä»½ï¼‰
        june_data = combined_data[combined_data[date_col].dt.month == 6]
        print(f"æ‰¾åˆ° {len(june_data)} æ¡6æœˆä»½è®°å½•")
        
        if len(june_data) == 0:
            # æ‰“å°å„æœˆä»½æ•°æ®é‡ç»Ÿè®¡
            month_counts = combined_data[date_col].dt.month.value_counts().sort_index()
            print("å„æœˆä»½æ•°æ®é‡ç»Ÿè®¡:\n", month_counts)
            
            # å°è¯•æ”¾å®½æ¡ä»¶ï¼šåŒ…å«"6"çš„æ—¥æœŸï¼ˆå¦‚2023-06æˆ–6æœˆç­‰ï¼‰
            june_condition = (
                combined_data[date_col].astype(str).str.contains('-06-|/06/|6æœˆ|Jun|June', case=False)
            )
            june_data = combined_data[june_condition]
            print(f"æ”¾å®½æ¡ä»¶åæ‰¾åˆ° {len(june_data)} æ¡6æœˆç›¸å…³è®°å½•")
        
        return june_data
    
    def preprocess_data(self, data):
        """æ•°æ®é¢„å¤„ç†"""
        # è®°å½•åˆå§‹æ•°æ®é‡
        original_count = len(data)
        
        # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
        numeric_columns = ['preclose', 'open', 'high', 'low', 'close', 'vol', 'amount']
        for col in numeric_columns:
            if col in data.columns:
                # å…ˆè½¬æ¢ä¸ºå­—ç¬¦ä¸²å†è½¬æ¢ä¸ºæ•°å­—ï¼Œé¿å…æ··åˆç±»å‹é—®é¢˜
                data[col] = pd.to_numeric(data[col].astype(str), errors='coerce')
        
        # ä»…ç§»é™¤å…¨éƒ¨ç‰¹å¾ä¸ºNAçš„è¡Œ
        data = data.dropna(subset=numeric_columns, how='all')
        
        # ç§»é™¤ä»·æ ¼ä¸º0çš„è®°å½•ä½†ä¿ç•™æ¥è¿‘0çš„å°æ•°
        price_columns = ['preclose', 'open', 'high', 'low', 'close']
        for col in price_columns:
            if col in data.columns:
                data = data[data[col] > 0.001]  # å…è®¸å¾®å°ä»·æ ¼
        
        # æ‰“å°è¿‡æ»¤ä¿¡æ¯
        filtered_count = original_count - len(data)
        print(f"æ•°æ®é¢„å¤„ç†è¿‡æ»¤äº† {filtered_count} æ¡è®°å½•ï¼Œä¿ç•™ {len(data)} æ¡")
        
        return data
            
    def load_model(self, model_path='models/best_enhanced_model.pth'):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print("åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹...")
        
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        # åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹
        # ä¿®æ”¹åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹çš„ä»£ç 
        checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)        
        # åˆ›å»ºæ¨¡å‹
        self.model = TransformerStockPredictor(
            input_dim=6,
            d_model=128,
            nhead=8,
            num_layers=4,
            dim_feedforward=512,
            sequence_length=30,
            dropout=0.1
        ).to(self.device)
        
        # åŠ è½½æ¨¡å‹æƒé‡
        self.model.load_state_dict(checkpoint['model_state_dict'])
        
        # åŠ è½½æ ‡å‡†åŒ–å™¨
        self.scaler = checkpoint['scaler']
        
        self.model.eval()
        print("æ¨¡å‹åŠ è½½å®Œæˆ!")
        
        return self.model
    
    def create_prediction_sequences(self, data, sequence_length=30):
        """ä¸ºé¢„æµ‹åˆ›å»ºåºåˆ—æ•°æ®"""
        print("åˆ›å»ºé¢„æµ‹åºåˆ—...")
        
        feature_columns = ['open', 'high', 'low', 'close', 'vol', 'amount']
        
        # æ ‡å‡†åŒ–æ•°æ®
        features = data[feature_columns].values
        if self.scaler is None:
            raise RuntimeError("æ ‡å‡†åŒ–å™¨scaleræœªåŠ è½½ï¼Œæ— æ³•è¿›è¡Œç‰¹å¾æ ‡å‡†åŒ–ã€‚è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦åŒ…å«scalerã€‚")
        features_scaled = self.scaler.transform(features)
        
        # æ›´æ–°æ•°æ®
        data_scaled = data.copy()
        data_scaled[feature_columns] = features_scaled
        
        sequences = []
        metadata = []
        
        # æŒ‰è‚¡ç¥¨ä»£ç åˆ†ç»„
        for secucode in data_scaled['secucode'].unique():
            stock_data = data_scaled[data_scaled['secucode'] == secucode].sort_values('tradingday')
            
            if len(stock_data) < sequence_length:
                continue
            
            # å–æœ€å30å¤©çš„æ•°æ®ä½œä¸ºåºåˆ—
            last_sequence = stock_data[feature_columns].values[-sequence_length:]
            sequences.append(last_sequence)
            
            # å­˜å‚¨å…ƒæ•°æ®
            metadata.append({
                'secucode': secucode,
                'last_date': stock_data.iloc[-1]['tradingday'],
                'last_close': stock_data.iloc[-1]['close']
            })
        
        sequences = np.array(sequences, dtype=np.float32)
        
        print(f"åˆ›å»ºäº† {len(sequences)} ä¸ªé¢„æµ‹åºåˆ—")
        return sequences, metadata
    
    def predict_june_prices(self, test_data):
        """é¢„æµ‹6æœˆä»½çš„æ”¶ç›˜ä»·"""
        print("å¼€å§‹é¢„æµ‹6æœˆä»½æ”¶ç›˜ä»·...")
        
        # åˆ›å»ºé¢„æµ‹åºåˆ—
        sequences, metadata = self.create_prediction_sequences(test_data)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        sequences_tensor = torch.tensor(sequences)
        dataset = torch.utils.data.TensorDataset(sequences_tensor)
        dataloader = DataLoader(dataset, batch_size=128, shuffle=False)
        
        # é¢„æµ‹
        predictions = []
        
        with torch.no_grad():
            for batch in tqdm(dataloader, desc="é¢„æµ‹ä¸­"):
                batch_sequences = batch[0].to(self.device)
                if self.model is None:
                    raise RuntimeError("æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•è¿›è¡Œé¢„æµ‹ã€‚è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ã€‚")
                batch_predictions = self.model(batch_sequences)
                predictions.extend(batch_predictions.cpu().numpy())
        
        # åæ ‡å‡†åŒ–é¢„æµ‹ç»“æœ
        predictions = np.array(predictions)
        dummy_data = np.zeros((len(predictions), 6))
        dummy_data[:, 3] = predictions  # closeä»·æ ¼åœ¨ç¬¬4åˆ—ï¼ˆç´¢å¼•3ï¼‰
        if self.scaler is None:
            raise RuntimeError("æ ‡å‡†åŒ–å™¨scaleræœªåŠ è½½ï¼Œæ— æ³•åæ ‡å‡†åŒ–é¢„æµ‹ç»“æœã€‚è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦åŒ…å«scalerã€‚")
        predictions_unscaled = self.scaler.inverse_transform(dummy_data)[:, 3]
        
        # åˆ›å»ºé¢„æµ‹ç»“æœDataFrame
        prediction_results = []
        for i, meta in enumerate(metadata):
            prediction_results.append({
                'secucode': meta['secucode'],
                'tradingday': meta['last_date'],
                'predicted_close': predictions_unscaled[i],
                'last_actual_close': meta['last_close'],
                'prediction_change': (predictions_unscaled[i] - meta['last_close']) / meta['last_close']
            })
        
        predictions_df = pd.DataFrame(prediction_results)
        # åªä¿ç•™2025å¹´6æœˆçš„é¢„æµ‹ç»“æœ
        june_predictions = predictions_df[predictions_df['tradingday'].astype(str).str.startswith('202506')].copy()
        # ä¿å­˜ä¸º6æœˆä»½é¢„æµ‹
        june_pred_path = os.path.join(self.results_dir, 'june_predictions.csv')
        june_predictions.to_csv(june_pred_path, index=False, encoding='utf-8')
        print(f"6æœˆä»½é¢„æµ‹ç»“æœå·²ä¿å­˜: {june_pred_path}")
        print(f"é¢„æµ‹å®Œæˆï¼Œå…±é¢„æµ‹ {len(june_predictions)} æ¡6æœˆæ•°æ®")
        return june_predictions
    
    def evaluate_predictions(self, predictions_df, actual_data):
        """è¯„ä¼°é¢„æµ‹ç»“æœ"""
        print("è¯„ä¼°é¢„æµ‹ç»“æœ...")
        
        # åˆå¹¶é¢„æµ‹å’Œå®é™…æ•°æ®
        # è®¡ç®—6æœˆä»½å¹³å‡æ”¶ç›˜ä»·ä½œä¸ºå®é™…å€¼
        actual_avg = actual_data.groupby('secucode')['close'].mean().reset_index()
        actual_avg.columns = ['secucode', 'actual_avg_close']
        
        # åˆå¹¶æ•°æ®
        evaluation_data = pd.merge(predictions_df, actual_avg, on='secucode', how='inner')
        
        if len(evaluation_data) == 0:
            print("è­¦å‘Šï¼šæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„è‚¡ç¥¨ä»£ç è¿›è¡Œè¯„ä¼°")
            return None
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        predicted = evaluation_data['predicted_close'].values
        actual = evaluation_data['actual_avg_close'].values
        predicted = np.asarray(predicted)
        actual = np.asarray(actual)
        
        mse = mean_squared_error(actual, predicted)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(actual, predicted)
        
        # è®¡ç®—ç›¸å¯¹è¯¯å·®
        relative_errors = np.abs((predicted - actual) / actual)
        mape = np.mean(relative_errors) * 100
        
        # è®¡ç®—RÂ²
        r2 = r2_score(actual, predicted)
        
        # è®¡ç®—æ–¹å‘å‡†ç¡®ç‡
        predicted_direction = np.sign(evaluation_data['prediction_change'])
        actual_direction = np.sign((actual - evaluation_data['last_actual_close']) / evaluation_data['last_actual_close'])
        direction_accuracy = np.mean(predicted_direction == actual_direction)
        
        # ä»·æ ¼åŒºé—´å‡†ç¡®ç‡
        def calculate_price_range_accuracy(predicted, actual, tolerance=0.05):
            """è®¡ç®—ä»·æ ¼åœ¨å®¹å¿èŒƒå›´å†…çš„å‡†ç¡®ç‡"""
            within_range = np.abs((predicted - actual) / actual) <= tolerance
            return np.mean(within_range)
        
        price_accuracy_5 = calculate_price_range_accuracy(predicted, actual, 0.05)
        price_accuracy_10 = calculate_price_range_accuracy(predicted, actual, 0.10)
        price_accuracy_20 = calculate_price_range_accuracy(predicted, actual, 0.20)
        
        # å‡†å¤‡è¯„ä¼°ç»“æœ
        evaluation_results = {
            'total_stocks': len(evaluation_data),
            'mse': mse,
            'rmse': rmse,
            'mae': mae,
            'mape': mape,
            'r2': r2,
            'direction_accuracy': direction_accuracy,
            'price_accuracy_5pct': price_accuracy_5,
            'price_accuracy_10pct': price_accuracy_10,
            'price_accuracy_20pct': price_accuracy_20,
            'mean_predicted_price': np.mean(predicted),
            'mean_actual_price': np.mean(actual),
            'std_predicted_price': np.std(predicted),
            'std_actual_price': np.std(actual)
        }
        
        print(f"è¯„ä¼°å®Œæˆï¼Œå…±è¯„ä¼° {len(evaluation_data)} åªè‚¡ç¥¨")
        return evaluation_results, evaluation_data
    
    def generate_report(self, evaluation_results, evaluation_data):
        """ç”Ÿæˆè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š"""
        print("ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
        
        # åˆ›å»ºæŠ¥å‘Š
        report = []
        report.append("="*80)
        report.append("è‚¡ç¥¨ä»·æ ¼é¢„æµ‹æ¨¡å‹è¯„ä¼°æŠ¥å‘Š")
        report.append("="*80)
        report.append(f"è¯„ä¼°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"é¢„æµ‹ç›®æ ‡: 2025å¹´6æœˆä»½è‚¡ç¥¨æ”¶ç›˜ä»·")
        report.append(f"è¯„ä¼°è‚¡ç¥¨æ•°é‡: {evaluation_results['total_stocks']}")
        report.append("")
        
        # åŸºæœ¬ç»Ÿè®¡æŒ‡æ ‡
        report.append("1. åŸºæœ¬ç»Ÿè®¡æŒ‡æ ‡")
        report.append("-" * 40)
        report.append(f"å‡æ–¹è¯¯å·® (MSE): {evaluation_results['mse']:.6f}")
        report.append(f"å‡æ–¹æ ¹è¯¯å·® (RMSE): {evaluation_results['rmse']:.6f}")
        report.append(f"å¹³å‡ç»å¯¹è¯¯å·® (MAE): {evaluation_results['mae']:.6f}")
        report.append(f"å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·® (MAPE): {evaluation_results['mape']:.2f}%")
        report.append(f"å†³å®šç³»æ•° (RÂ²): {evaluation_results['r2']:.6f}")
        report.append("")
        
        # å‡†ç¡®ç‡æŒ‡æ ‡
        report.append("2. å‡†ç¡®ç‡æŒ‡æ ‡")
        report.append("-" * 40)
        report.append(f"æ–¹å‘é¢„æµ‹å‡†ç¡®ç‡: {evaluation_results['direction_accuracy']:.2%}")
        report.append(f"ä»·æ ¼é¢„æµ‹å‡†ç¡®ç‡ (Â±5%): {evaluation_results['price_accuracy_5pct']:.2%}")
        report.append(f"ä»·æ ¼é¢„æµ‹å‡†ç¡®ç‡ (Â±10%): {evaluation_results['price_accuracy_10pct']:.2%}")
        report.append(f"ä»·æ ¼é¢„æµ‹å‡†ç¡®ç‡ (Â±20%): {evaluation_results['price_accuracy_20pct']:.2%}")
        report.append("")
        
        # ä»·æ ¼ç»Ÿè®¡
        report.append("3. ä»·æ ¼ç»Ÿè®¡")
        report.append("-" * 40)
        report.append(f"é¢„æµ‹ä»·æ ¼å‡å€¼: {evaluation_results['mean_predicted_price']:.2f}")
        report.append(f"å®é™…ä»·æ ¼å‡å€¼: {evaluation_results['mean_actual_price']:.2f}")
        report.append(f"é¢„æµ‹ä»·æ ¼æ ‡å‡†å·®: {evaluation_results['std_predicted_price']:.2f}")
        report.append(f"å®é™…ä»·æ ¼æ ‡å‡†å·®: {evaluation_results['std_actual_price']:.2f}")
        report.append("")
        
        # æ¨¡å‹æ€§èƒ½è¯„ä¼°
        report.append("4. æ¨¡å‹æ€§èƒ½è¯„ä¼°")
        report.append("-" * 40)
        
        # åŸºäºä¸åŒæŒ‡æ ‡çš„è¯„ä¼°
        if evaluation_results['r2'] > 0.8:
            r2_grade = "ä¼˜ç§€"
        elif evaluation_results['r2'] > 0.6:
            r2_grade = "è‰¯å¥½"
        elif evaluation_results['r2'] > 0.4:
            r2_grade = "ä¸€èˆ¬"
        else:
            r2_grade = "è¾ƒå·®"
        
        if evaluation_results['mape'] < 5:
            mape_grade = "ä¼˜ç§€"
        elif evaluation_results['mape'] < 10:
            mape_grade = "è‰¯å¥½"
        elif evaluation_results['mape'] < 20:
            mape_grade = "ä¸€èˆ¬"
        else:
            mape_grade = "è¾ƒå·®"
        
        if evaluation_results['direction_accuracy'] > 0.6:
            direction_grade = "ä¼˜ç§€"
        elif evaluation_results['direction_accuracy'] > 0.55:
            direction_grade = "è‰¯å¥½"
        elif evaluation_results['direction_accuracy'] > 0.5:
            direction_grade = "ä¸€èˆ¬"
        else:
            direction_grade = "è¾ƒå·®"
        
        report.append(f"RÂ²è¯„çº§: {r2_grade}")
        report.append(f"MAPEè¯„çº§: {mape_grade}")
        report.append(f"æ–¹å‘å‡†ç¡®ç‡è¯„çº§: {direction_grade}")
        report.append("")
        
        # æ€»ä½“è¯„ä¼°
        report.append("5. æ€»ä½“è¯„ä¼°")
        report.append("-" * 40)
        
        grades = [r2_grade, mape_grade, direction_grade]
        if grades.count('ä¼˜ç§€') >= 2:
            overall_grade = "ä¼˜ç§€"
        elif grades.count('è‰¯å¥½') >= 2:
            overall_grade = "è‰¯å¥½"
        elif grades.count('ä¸€èˆ¬') >= 2:
            overall_grade = "ä¸€èˆ¬"
        else:
            overall_grade = "è¾ƒå·®"
        
        report.append(f"æ¨¡å‹æ€»ä½“è¯„çº§: {overall_grade}")
        report.append("")
        
        # å»ºè®®
        report.append("6. æ”¹è¿›å»ºè®®")
        report.append("-" * 40)
        
        if evaluation_results['r2'] < 0.6:
            report.append("- è€ƒè™‘å¢åŠ æ›´å¤šæŠ€æœ¯æŒ‡æ ‡ä½œä¸ºç‰¹å¾")
            report.append("- å°è¯•è°ƒæ•´æ¨¡å‹æ¶æ„æˆ–è¶…å‚æ•°")
        
        if evaluation_results['mape'] > 15:
            report.append("- è€ƒè™‘å¯¹å¼‚å¸¸å€¼è¿›è¡Œæ›´å¥½çš„å¤„ç†")
            report.append("- å¯èƒ½éœ€è¦æ›´å¤šçš„è®­ç»ƒæ•°æ®")
        
        if evaluation_results['direction_accuracy'] < 0.55:
            report.append("- è€ƒè™‘ä½¿ç”¨åˆ†ç±»æ¨¡å‹æ¥é¢„æµ‹ä»·æ ¼æ–¹å‘")
            report.append("- å¢åŠ å¸‚åœºæƒ…ç»ªæŒ‡æ ‡")
        
        report.append("")
        report.append("="*80)
        
        # ä¿å­˜æŠ¥å‘Š
        report_text = "\n".join(report)
        with open(os.path.join(self.results_dir, 'prediction_evaluation_report.txt'), 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        print("è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜")
        return report_text
    
    def create_visualizations(self, evaluation_data):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        print("åˆ›å»ºå¯è§†åŒ–å›¾è¡¨...")
        
        # è®¾ç½®å­—ä½“ï¼Œä¼˜å…ˆç”¨ç³»ç»Ÿé»˜è®¤å­—ä½“ï¼Œæ‰¾ä¸åˆ°SimHei/Arial Unicode MSæ—¶è‡ªåŠ¨é™çº§
        import matplotlib
        try:
            plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans', 'sans-serif']
        except Exception:
            plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'sans-serif']
        plt.rcParams['axes.unicode_minus'] = False
        
        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # 1. é¢„æµ‹vså®é™…æ•£ç‚¹å›¾
        axes[0, 0].scatter(evaluation_data['actual_avg_close'], 
                          evaluation_data['predicted_close'], 
                          alpha=0.6, s=30)
        axes[0, 0].plot([evaluation_data['actual_avg_close'].min(), 
                        evaluation_data['actual_avg_close'].max()],
                       [evaluation_data['actual_avg_close'].min(), 
                        evaluation_data['actual_avg_close'].max()], 
                       'r--', lw=2)
        axes[0, 0].set_xlabel('å®é™…æ”¶ç›˜ä»·')
        axes[0, 0].set_ylabel('é¢„æµ‹æ”¶ç›˜ä»·')
        axes[0, 0].set_title('é¢„æµ‹vså®é™…ä»·æ ¼æ•£ç‚¹å›¾')
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾
        errors = evaluation_data['predicted_close'] - evaluation_data['actual_avg_close']
        axes[0, 1].hist(errors, bins=50, alpha=0.7, edgecolor='black')
        axes[0, 1].set_xlabel('é¢„æµ‹è¯¯å·®')
        axes[0, 1].set_ylabel('é¢‘æ•°')
        axes[0, 1].set_title('é¢„æµ‹è¯¯å·®åˆ†å¸ƒ')
        axes[0, 1].axvline(x=0, color='red', linestyle='--', alpha=0.8)
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. ç›¸å¯¹è¯¯å·®åˆ†å¸ƒ
        relative_errors = np.abs((evaluation_data['predicted_close'] - evaluation_data['actual_avg_close']) / 
                                evaluation_data['actual_avg_close']) * 100
        axes[0, 2].hist(relative_errors, bins=50, alpha=0.7, edgecolor='black')
        axes[0, 2].set_xlabel('ç›¸å¯¹è¯¯å·® (%)')
        axes[0, 2].set_ylabel('é¢‘æ•°')
        axes[0, 2].set_title('ç›¸å¯¹è¯¯å·®åˆ†å¸ƒ')
        axes[0, 2].axvline(x=5, color='red', linestyle='--', alpha=0.8, label='5%')
        axes[0, 2].axvline(x=10, color='orange', linestyle='--', alpha=0.8, label='10%')
        axes[0, 2].legend()
        axes[0, 2].grid(True, alpha=0.3)
        
        # 4. ä»·æ ¼åŒºé—´åˆ†å¸ƒ
        price_ranges = ['<10', '10-20', '20-50', '50-100', '>100']
        actual_counts = []
        predicted_counts = []
        
        for range_str in price_ranges:
            if range_str == '<10':
                actual_count = len(evaluation_data[evaluation_data['actual_avg_close'] < 10])
                predicted_count = len(evaluation_data[evaluation_data['predicted_close'] < 10])
            elif range_str == '10-20':
                actual_count = len(evaluation_data[(evaluation_data['actual_avg_close'] >= 10) & 
                                                 (evaluation_data['actual_avg_close'] < 20)])
                predicted_count = len(evaluation_data[(evaluation_data['predicted_close'] >= 10) & 
                                                    (evaluation_data['predicted_close'] < 20)])
            elif range_str == '20-50':
                actual_count = len(evaluation_data[(evaluation_data['actual_avg_close'] >= 20) & 
                                                 (evaluation_data['actual_avg_close'] < 50)])
                predicted_count = len(evaluation_data[(evaluation_data['predicted_close'] >= 20) & 
                                                    (evaluation_data['predicted_close'] < 50)])
            elif range_str == '50-100':
                actual_count = len(evaluation_data[(evaluation_data['actual_avg_close'] >= 50) & 
                                                 (evaluation_data['actual_avg_close'] < 100)])
                predicted_count = len(evaluation_data[(evaluation_data['predicted_close'] >= 50) & 
                                                    (evaluation_data['predicted_close'] < 100)])
            else:  # >100
                actual_count = len(evaluation_data[evaluation_data['actual_avg_close'] >= 100])
                predicted_count = len(evaluation_data[evaluation_data['predicted_close'] >= 100])
            
            actual_counts.append(actual_count)
            predicted_counts.append(predicted_count)
        
        x = np.arange(len(price_ranges))
        width = 0.35
        
        axes[1, 0].bar(x - width/2, actual_counts, width, label='å®é™…', alpha=0.8)
        axes[1, 0].bar(x + width/2, predicted_counts, width, label='é¢„æµ‹', alpha=0.8)
        axes[1, 0].set_xlabel('ä»·æ ¼åŒºé—´')
        axes[1, 0].set_ylabel('è‚¡ç¥¨æ•°é‡')
        axes[1, 0].set_title('ä»·æ ¼åŒºé—´åˆ†å¸ƒå¯¹æ¯”')
        axes[1, 0].set_xticks(x)
        axes[1, 0].set_xticklabels(price_ranges)
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 5. æ–¹å‘é¢„æµ‹å‡†ç¡®ç‡
        predicted_direction = np.sign(evaluation_data['prediction_change'])
        actual_direction = np.sign((evaluation_data['actual_avg_close'] - evaluation_data['last_actual_close']) / 
                                  evaluation_data['last_actual_close'])
        
        direction_correct = (predicted_direction == actual_direction).sum()
        direction_wrong = len(evaluation_data) - direction_correct
        
        direction_labels = ['æ­£ç¡®', 'é”™è¯¯']
        direction_values = [direction_correct, direction_wrong]
        colors = ['green', 'red']
        
        axes[1, 1].pie(direction_values, labels=direction_labels, colors=colors, autopct='%1.1f%%', startangle=90)
        axes[1, 1].set_title('æ–¹å‘é¢„æµ‹å‡†ç¡®ç‡')
        
        # 6. é¢„æµ‹è¯¯å·®ä¸å®é™…ä»·æ ¼çš„å…³ç³»
        axes[1, 2].scatter(evaluation_data['actual_avg_close'], 
                          relative_errors, 
                          alpha=0.6, s=30)
        axes[1, 2].set_xlabel('å®é™…æ”¶ç›˜ä»·')
        axes[1, 2].set_ylabel('ç›¸å¯¹è¯¯å·® (%)')
        axes[1, 2].set_title('é¢„æµ‹è¯¯å·®ä¸å®é™…ä»·æ ¼çš„å…³ç³»')
        axes[1, 2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.results_dir, 'prediction_evaluation_charts.png'), 
                   dpi=300, bbox_inches='tight')
        plt.show()
        
        print("å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜")
    
    def save_detailed_results(self, evaluation_data):
        """ä¿å­˜è¯¦ç»†çš„é¢„æµ‹ç»“æœ"""
        print("ä¿å­˜è¯¦ç»†ç»“æœ...")
        
        # è®¡ç®—é¢å¤–çš„ç»Ÿè®¡ä¿¡æ¯
        evaluation_data['absolute_error'] = np.abs(evaluation_data['predicted_close'] - evaluation_data['actual_avg_close'])
        evaluation_data['relative_error'] = np.abs((evaluation_data['predicted_close'] - evaluation_data['actual_avg_close']) / 
                                                  evaluation_data['actual_avg_close']) * 100
        evaluation_data['predicted_direction'] = np.sign(evaluation_data['prediction_change'])
        evaluation_data['actual_direction'] = np.sign((evaluation_data['actual_avg_close'] - evaluation_data['last_actual_close']) / 
                                                     evaluation_data['last_actual_close'])
        evaluation_data['direction_correct'] = (evaluation_data['predicted_direction'] == evaluation_data['actual_direction'])
        
        # æŒ‰ç›¸å¯¹è¯¯å·®æ’åº
        evaluation_data_sorted = evaluation_data.sort_values('relative_error')
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        evaluation_data_sorted.to_csv(os.path.join(self.results_dir, 'detailed_prediction_results.csv'), 
                                     index=False, encoding='utf-8')
        
        # ä¿å­˜æœ€å¥½å’Œæœ€å·®çš„é¢„æµ‹ç»“æœ
        best_predictions = evaluation_data_sorted.head(20)
        worst_predictions = evaluation_data_sorted.tail(20)
        
        best_predictions.to_csv(os.path.join(self.results_dir, 'best_predictions.csv'), 
                               index=False, encoding='utf-8')
        worst_predictions.to_csv(os.path.join(self.results_dir, 'worst_predictions.csv'), 
                                index=False, encoding='utf-8')
        
        print("è¯¦ç»†ç»“æœå·²ä¿å­˜")
    
    def run_prediction_evaluation(self):
        """è¿è¡Œå®Œæ•´çš„é¢„æµ‹è¯„ä¼°æµç¨‹"""
        print("="*80)
        print("è‚¡ç¥¨ä»·æ ¼é¢„æµ‹æ¨¡å‹è¯„ä¼°å¼€å§‹")
        print("="*80)
        
        try:
            # 1. åŠ è½½æ¨¡å‹å’Œæ•°æ®
            self.load_model()
            test_data = self.load_test_data()
            actual_data = self.load_actual_data()
    
            # 2. æ£€æŸ¥æ•°æ®æ˜¯å¦æœ‰æ•ˆ
            if test_data.empty or actual_data.empty:
                print("é”™è¯¯ï¼šæµ‹è¯•æ•°æ®æˆ–å®é™…æ•°æ®ä¸ºç©º")
                return None, None
    
            # 3. æ‰§è¡Œé¢„æµ‹
            predictions_df = self.predict_june_prices(test_data)
            if predictions_df.empty:
                print("é”™è¯¯ï¼šæœªç”Ÿæˆä»»ä½•é¢„æµ‹ç»“æœ")
                return None, None
    
            # 4. è¯„ä¼°å¹¶å¤„ç†ç©ºç»“æœ
            evaluation_results, evaluation_data = self.evaluate_predictions(predictions_df, actual_data)
            if not evaluation_results or evaluation_data.empty:
                print("è¯„ä¼°å¤±è´¥ï¼šæ— æœ‰æ•ˆæ•°æ®")
                return None, None
    
            # 5. ç”ŸæˆæŠ¥å‘Šå’Œå¯è§†åŒ–
            self.generate_report(evaluation_results, evaluation_data)
            self.create_visualizations(evaluation_data)
            return evaluation_results, evaluation_data
            
            # 6. ç”ŸæˆæŠ¥å‘Š
            report = self.generate_report(evaluation_results, evaluation_data)
            
            # 7. åˆ›å»ºå¯è§†åŒ–
            self.create_visualizations(evaluation_data)
            
            # 8. ä¿å­˜è¯¦ç»†ç»“æœ
            self.save_detailed_results(evaluation_data)
            
            # 9. ä¿å­˜è¯„ä¼°ç»“æœJSON
            with open(os.path.join(self.results_dir, 'evaluation_results.json'), 'w', encoding='utf-8') as f:
                json.dump(evaluation_results, f, indent=2, ensure_ascii=False)
            
            print("="*80)
            print("é¢„æµ‹è¯„ä¼°å®Œæˆ!")
            print(f"è¯„ä¼°æŠ¥å‘Šä¿å­˜åœ¨: {os.path.join(self.results_dir, 'prediction_evaluation_report.txt')}")
            print(f"å¯è§†åŒ–å›¾è¡¨ä¿å­˜åœ¨: {os.path.join(self.results_dir, 'prediction_evaluation_charts.png')}")
            print(f"è¯¦ç»†ç»“æœä¿å­˜åœ¨: {os.path.join(self.results_dir, 'detailed_prediction_results.csv')}")
            print("="*80)
            
            # æ‰“å°ç®€è¦ç»“æœ
            print("\nç®€è¦è¯„ä¼°ç»“æœ:")
            print(f"è¯„ä¼°è‚¡ç¥¨æ•°é‡: {evaluation_results['total_stocks']}")
            print(f"RÂ²: {evaluation_results['r2']:.4f}")
            print(f"MAPE: {evaluation_results['mape']:.2f}%")
            print(f"æ–¹å‘å‡†ç¡®ç‡: {evaluation_results['direction_accuracy']:.2%}")
            print(f"ä»·æ ¼å‡†ç¡®ç‡ (Â±5%): {evaluation_results['price_accuracy_5pct']:.2%}")
            print(f"ä»·æ ¼å‡†ç¡®ç‡ (Â±10%): {evaluation_results['price_accuracy_10pct']:.2%}")
            
            return evaluation_results, evaluation_data
            
        except Exception as e:
            print(f"é¢„æµ‹è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            raise

# ä¸»ç¨‹åº
if __name__ == "__main__":
    # åˆ›å»ºé¢„æµ‹è¯„ä¼°å™¨å®ä¾‹
    evaluator = StockPredictor()
    
    # è¿è¡Œé¢„æµ‹è¯„ä¼°
    try:
        results, data = evaluator.run_prediction_evaluation()
        print("\né¢„æµ‹è¯„ä¼°æµç¨‹å®Œæˆ!")
        
        # æ‰“å°ä¸€äº›ç»Ÿè®¡ä¿¡æ¯
        if results:
            print("\n=== æœ€ç»ˆè¯„ä¼°æ‘˜è¦ ===")
            print(f"æ€»ä½“è¯„ä¼°: åŸºäºRÂ²={results['r2']:.4f}, MAPE={results['mape']:.2f}%, æ–¹å‘å‡†ç¡®ç‡={results['direction_accuracy']:.2%}")
            
            if results['r2'] > 0.6 and results['mape'] < 15 and results['direction_accuracy'] > 0.55:
                print("ğŸ‰ æ¨¡å‹è¡¨ç°è‰¯å¥½ï¼")
            elif results['r2'] > 0.4 and results['mape'] < 25 and results['direction_accuracy'] > 0.5:
                print("âœ… æ¨¡å‹è¡¨ç°ä¸€èˆ¬ï¼Œä½†ä»æœ‰å‚è€ƒä»·å€¼")
            else:
                print("âš ï¸ æ¨¡å‹è¡¨ç°è¾ƒå·®ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–")
                
    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å’Œæ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œè·¯å¾„æ˜¯å¦æ­£ç¡®")